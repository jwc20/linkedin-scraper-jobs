company_name,job_title,job_link,job_description,extracted_skills,date_scraped
Intuit,Data Engineer 2 (Mailchimp),https://www.linkedin.com/jobs/view/data-engineer-2-mailchimp-at-intuit-3903939352?position=1&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=sukoUviM6bBAblxPAC8Nvg%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Qualcomm,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-qualcomm-3889559420?position=2&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=LbCRzaKVJFevVxkEpT%2F7XQ%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Saragossa,"Data Engineer – Hedge Fund - $150,000-200,000 Salary + Bonuses",https://www.linkedin.com/jobs/view/data-engineer-%E2%80%93-hedge-fund-%24150-000-200-000-salary-%2B-bonuses-at-saragossa-3886268515?position=3&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=ysM5C20wypt6VSnY4RTqXQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Big tech is still pretty fragile and career outlook right now is very uncertain. What about working at a hedge fund where their growth is almost certain?This data engineering team hired multiple people last year – most from big-tech – and plan to do the same this year.A large portion of your work will technically be non-finance, with the main finance focused work completed by other teams. The real-world, complex data engineering problems are all yours.You’ll be joining a team that builds and has built thousands of ingestion pipelines, with data coming from every single potential avenue it possibly can. To get the best understanding of an investment, all data points are considered. You’ll bring the data into the business, build the data infrastructure around it and deliver the data to the right person in the business to build their investment models.Tech is also completely flexible. Most of the work is done within Python, C# and Scala with a range of databases. Snowflake is widely used, as are Docker and Kubernetes for containerisation. ETL and ELT tech are also used every day, primarily Airflow, Spark, Hive and a lot more.You’ll need to come from a strong academic background with some commercial experience in a data heavy software or data engineering role. Financial experience of some kind would be a huge plus, but absolutely not a requirement. Experience with as much of the tech stack above is preferred and most likely, but you don’t need experience with all. Picking up new tech will be part of your job.This is a $60 billion plus fund, where being at the cutting edge of technology in general, let alone finance, is key for them. With this amount of ambition, it’s why their bonuses can be some of the biggest in the industry. First year total compensation could be as high as $400,000 and this has the potential to get better year on year, based on your performance.When in the office you have a fully catered canteen with a Michelin standard chef, barista coffee, a gym and more.Want to explore (or to continue exploring) high-tech in the Hedge Fund world?No up-to-date resume required initially, we can work on updating it together if need be before submitting. Show more Show less Seniority level Associate Employment type Full-time Job function Information Technology, Engineering, and Finance Industries Software Development, Data Infrastructure and Analytics, and Technology, Information and Media","['Snowflake', 'Airflow']",2024-04-21 15:53:59.643028
Staples,Data Engineer III,https://www.linkedin.com/jobs/view/data-engineer-iii-at-staples-3889804536?position=4&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=tY5K68i%2By5hYjg4bHdCWWQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Direct message the job poster from Staples Loren Jenkins Loren Jenkins Hiring Technology Professionals for SDS - Staples Digital Solutions! Staples is business to business. You’re what binds us together.Our digital solutions team is more than a traditional IT organization. We are a team of passionate, collaborative, agile, inventive, customer-centric, results-oriented problem solvers. We are intellectually curious, love advancements in technology and seek to adapt technologies to drive Staples forward. We anticipate the needs of our customers and business partners and deliver reliable, customer-centric technology services.What you’ll be doing:Design, build and test end to end data pipeline including data ingestion (streaming, events, and batch), data integration, data curation.Build and support data platform on the cloud.Define and implement automation of jobs and testing.Optimize the data pipeline to support workloads and use cases.Support mission critical applications and near real time data needs from the data platform. Build and support reusable framework to ingest, integration and provision data.Capture and publish metadata and new data to subscribed users. Work collaboratively with product managers, data scientists as well as business partners and actively participate in solution thinking sessions.Participate in design and code reviews.Lead data disaster recovery planning and execution with the disaster recovery team and internal team members. Determine risk of various configurations and options for data administration and can communicate issues to managers and team.Lead and develop internal standards for data administration and data system documentation.Develop physical data design. Define, document, and execute, referential integrity requirements, constraints, and techniques. Identify index requirements and determine storage requirements. What you bring to the table:Strong knowledge of Data Management and Data AnalysisStrong interpersonal and communication skillsRetail/e-comm experience What’s needed- Basic Qualifications:Bachelor’s Degree 5+ years of Python, Pyspark experience5+ years of Snowflake experience5+ years of SQL experience5 years of Databricks experience2+ years of experience with cloud computing (Microsoft Azure) 5+ years of experience in Agile practices What’s needed- Preferred Qualifications:Master’s Degree Bias for action and a natural ability to work around impediments. Experience building multiple concurrent projects of high technical complexity We Offer:Inclusive culture with associate-led Business Resource GroupsFlexible PTO (22 days) and Holiday ScheduleOnline and Retail Discounts, Company Match 401(k), Physical and Mental Health Wellness programs, and more! Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Technology, Information and Media and IT Services and IT Consulting","['Snowflake', 'Databricks']",2024-04-21 15:53:59.643028
Netflix,Data Engineer (L4) - Privacy,https://www.linkedin.com/jobs/view/data-engineer-l4-privacy-at-netflix-3877633696?position=5&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=O48hLkuJBN47Wp0A%2B%2F360g%3D%3D&trk=public_jobs_jserp-result_search-card,"Netflix is enjoyed by more than 230 million members globally, entertaining new audiences every day. We are in the midst of major transformative developments for the Netflix product with the launch of an Advertising-supported plan, Games, and Live content. These new products, alongside our streaming service, have resulted in significant increases in the complexity and breadth of our internal data ecosystem. We manage one of the largest paid subscription businesses and are committed to handling our members’ data with a high degree of care towards appropriate use as well as compliance with local privacy laws and regulations in the 190+ countries where we operate.The Privacy and Legal Data Engineering pod builds scalable data management and extraction frameworks which are at the core of our ability to hold ourselves to the highest data privacy and hygiene standards in the industry.We are looking for a Data Engineer to help augment our ability to build these robust, scalable privacy-centric data frameworks. As part of this team, you will work on diverse data technologies such as Spark, Presto, Flink, Kafka, and others to build insightful, scalable and robust data pipelines; write ETL jobs to collect and aggregate data; and build high-quality data frameworks that enable appropriate handling of customer personal data.The ideal candidate will have a strong background in distributed data processing, have great data intuition, and share our passion for continuously improving the ways we handle data to make Netflix's data privacy posture better.Who You ArePassionate about consumer-centric data privacy and risk mitigation for the businessHighly proficient in at least one of Java, Python, or Scala with at least 2 years of software/data engineering experienceComfortable with advanced SQLExperienced in engineering data pipelines using big data technologies (Hive, Presto, Spark, Flink) on medium to large scale data setsExcel at taking requirements and implementing scalable data models and pipelines Excited about demonstrating excellence, learning new technologies, and growing your career What You Will DoEngineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data Develop a good understanding of the data ecosystem at Netflix from a privacy lens Partner with the privacy engineering teams to understand product goals and provide data that enables us to respond to customer and regulatory data requests Maintain and rethink existing datasets and pipelinesJoin a stunning team of data engineers who enhance our data privacy posture and mitigate legal and regulatory riskA Few More Things To KnowOur culture is unique and we live by our values, so it's worth learning more about Netflix at jobs.netflix.com/culture. We regularly share examples of our work on our tech blog. You will need to be comfortable working in the most agile of environments. Requirements will be vague. Iterations will be rapid. You will need to be nimble and take smart risks.Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top-of-market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $100,000 - $720,000.Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more details about our Benefits here.Netflix is a unique culture and environment. Learn more here.We are an equal opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service. Show more Show less Seniority level Not Applicable Employment type Full-time Job function Information Technology Industries Entertainment Providers, Technology, Information and Internet, and Movies, Videos, and Sound","['Kafka', 'NiFi', 'presto']",2024-04-21 15:53:59.643028
WHOOP,Data Engineer I,https://www.linkedin.com/jobs/view/data-engineer-i-at-whoop-3893345035?position=6&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=fD59e3U%2FNu%2BprXnnyvJJEQ%3D%3D&trk=public_jobs_jserp-result_search-card,"At WHOOP, we're on a mission to unlock human performance. WHOOP empowers members to perform at a higher level through a deeper understanding of their bodies and daily lives.This person will work on large datasets that drive new product features at WHOOP, which is a critical part of our innovative solution. You will keep an eye on the customer experience, ensuring that all algorithms that are created work across WHOOP’s diverse member population.ResponsibilitiesOrganizing, validating, and preprocessing sensor data;Developing tools for visualization and data interpretation;Identifying issues related to data and data quality;Creating validations and automation of tests for the data and algorithms;Contributing to developing and maintaining infrastructure for data processing; andDeveloping analytical tools and programs.QualificationsMaster of Science (or equivalent foreign degree) in Computer Science, Computer Engineering, or a closely related field and two (2) years of experience as a software engineer/developer involving data generation, extraction and integration.Experience, which may have been gained concurrently, must include one year in the following: Data warehousing, data modeling, and building ETL pipelines Working with Python and SQL to extract and transform large datasets Data analysis and visualization using Python Working with Cloud Computing solutions or similar tools Generating Data Quality ReportsThis role is based in the WHOOP office located in Boston, MA. The successful candidate must be prepared to relocate if necessary to work out of the Boston, MA office.WHOOP is an Equal Opportunity Employer and participates in E-verify to determine employment eligibility. Show more Show less Seniority level Entry level Employment type Full-time Job function Information Technology Industries Wellness and Fitness Services",[],2024-04-21 15:53:59.643028
LinkedIn,Senior Data Engineer - Data Science,https://www.linkedin.com/jobs/view/senior-data-engineer-data-science-at-linkedin-3860537886?position=7&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=j%2BOSswuC0G9SoE5NGnjpdQ%3D%3D&trk=public_jobs_jserp-result_search-card,"LinkedIn is the world’s largest professional network, built to help members of all backgrounds and experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters.This role will be based in Sunnyvale or San Francisco.At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. LinkedIn’s Data Science team leverages big data to empower business decisions and deliver data-driven insights, metrics, and tools in order to drive member engagement, business growth, and monetization efforts. With over 800 million members around the world, a focus on great user experience, and a mix of B2B and B2C programs, LinkedIn offers countless ways for an ambitious data engineer to have an impact and transform your career.We are now looking for a talented and driven individual to accelerate our efforts and be a major part of our data-centric culture. This person will work closely with various cross-functional teams such as product, marketing, sales, engineering, and operations to develop infrastructure and deliver tools or data structures that enable data-driven decision-making. Successful candidates will exhibit technical acumen and business savviness with a passion for making an impact by enabling both producers and consumers of data insight to work smarter.Responsibilities:● Work with a team of high-performing data science professionals, and cross-functional teams to identify business opportunities and build scalable data solutions.● Build data expertise, act like an owner for the company and manage complex data systems for a product or a group of products.● Perform all of the necessary data transformations to serve products that empower data-driven decision making.● Build and manage data pipelines, design and architect databases.● Establish efficient design and programming patterns for engineers as well as for non-technical partners.● Design, implement, integrate and document performant systems or components for data flows or applications that power analysis at a massive scale.● Ensure best practices and standards in our data ecosystem are shared across teams.● Understand the analytical objectives to make logical recommendations and drive informed actions.● Engage with internal platform teams to prototype and validate tools developed in-house to derive insight from very large datasets or automate complex algorithms.● Be a self-starter, Initiate and drive projects to completion with minimal guidance.● Contribute to engineering innovations that fuel LinkedIn’s vision and mission.Basic Qualifications:● Bachelor's Degree in a quantitative discipline: Computer science, Statistics, Operations Research, Informatics, Engineering, Applied Mathematics, Economics, etc.● 3+ years of relevant industry or relevant academia experience working with large amounts of data● Experience with SQL/Relational databases● Background in at least one programming languages (e.g., R, Python, Java, Scala, PHP, JavaScript)Preferred Qualifications:● BS and 5+ years of relevant work experience, MS and 3+ years of relevant work experience, or Ph.D. and 1+ years of relevant work/academia experience working with large amounts of data● MS or PhD in a quantitative discipline: statistics, operations research, computer science, informatics, engineering, applied mathematics, economics, etc.● Experience in developing data pipelines using Spark and Hive.● Experience with data modeling, ETL (Extraction, Transformation & Load) concepts, and patterns for efficient data governance. Experience with manipulating massive-scale structured and unstructured data.● Experience with using distributed data systems such as Spark and related technologies (Presto/Trino, Hive, etc.).● Experience with either data workflows/modeling, front-end engineering, or back-end engineering.● Deep understanding of technical and functional designs for relational and MPP Databases● Experience in data visualization and dashboard design including tools such as Tableau, R visualization packages, streamlit, D3, and other libraries, etc.● Knowledge of Unix and Unix-like systems, version control systems such as Git.Suggested Skills:● Distributed Systems● ETL● Data ModelingYou will Benefit from our Culture:We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levelsLinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $117,000.00 to $192,000.00 Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits.Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice Show more Show less Seniority level Not Applicable Employment type Full-time Job function Engineering Industries Technology, Information and Internet and IT Services and IT Consulting","['presto', 'Tableau']",2024-04-21 15:53:59.643028
Intuit,Data Engineer 2 (Mailchimp),https://www.linkedin.com/jobs/view/data-engineer-2-mailchimp-at-intuit-3903943032?position=8&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=7RzN%2Fj1bDRXAbMkwU0AH2w%3D%3D&trk=public_jobs_jserp-result_search-card,"OverviewIntuit is a global technology platform that helps consumers and small businesses overcome their most important financial challenges. Serving more than 100 million customers worldwide with TurboTax, Credit Karma, QuickBooks, and Mailchimp, we believe that everyone should have the opportunity to prosper. We never stop working to find new, innovative ways to make that possible.Intuit Mailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multi channel campaigns, CRM, and analytics tools.The Data Products team at Mailchimp is responsible for building capabilities on top of our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you’ll be building modeling data to feed analytical reports and data science models, optimizing and creating new ETL pipelines, or building data models in Looker or Qlik Sense. We have a lot of tools in our toolbelt, but all with the main goal of helping Mailchimp to continue to use data more effectively.What you'll bringStrong technical accomplishments in SQL and data analysis skillsExperience with data transformation orchestration tools like dbt or DataformBasic business intuition and ability to understand complex business systemsFamiliarity with additional transformation platforms like Apache Spark and BeamExperience with MPP databases such as BigQuery, Redshift, or Snowflake. We use BigQuery here at MailchimpExperience with visualization technologies like Looker, Tableau, or Qlik SenseExperience with GCP/AWS or other cloud providers is preferredFamiliarity with Python, Go, Java or another OOP language. Most of our tools are primarily built in PythonFamiliarity with ETL orchestration tools like Apache Airflow and Google DataflowHow you will leadPartner with teams across the business and third parties to understand their needs and come up with end-to-end data solutionsModel data to create core, governed data sets for the entire company to leverageBuild scalable transformation pipelines that enable teams to easily clean up, normalize, and govern data for broad consumptionManage and model data in visualization tools, to provide a collaborative data analytics platform for the companyBuild tools and processes to help make the right data accessible to the right peopleBecome a subject matter expert on the various datasets and tools we have at MailchimpWork with Data Stewards to better document our data Show more Show less Seniority level Associate Employment type Full-time Job function Information Technology Industries Software Development","['Snowflake', 'Redshift', 'BigQuery', 'Airflow', 'dbt', 'Tableau', 'Looker']",2024-04-21 15:53:59.643028
"Trepp, Inc.",Intern - Data Engineer,https://www.linkedin.com/jobs/view/intern-data-engineer-at-trepp-inc-3866288997?position=9&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=xxd6vG7JHktKSS8dTHKAuQ%3D%3D&trk=public_jobs_jserp-result_search-card,"(This position is based in Trepp's NYC office which operates in a hybrid work schedule) The Data Operations Group at Trepp is responsible for creating and developing our suite of software products responsible for our core applications. Being a member of the Data Operations Group allows you the flexibility to learn and hone your skills on various tools and technologies. You will have the opportunity of working with and creating services and pipelines used by teams including Structured Finance Modelers, Research, and Data Platform. Our products focus on data and analytics in structured finance (CMBS), collateralized loan obligations (CLO), commercial real estate (CRE), and banking & lending. This team works closely with our Product Groups to always refine & improve our product and feature sets, data capabilities and delivery, along with creating new Product Workflows. About this role: As a Data Engineer-Intern, you are responsible for helping the data group build quality products for Trepp’s data organization. The position requires technical skills and must be able to work collaboratively with a group of high performing individuals. The role involves data engineering skills to build reliable data pipelines with low latency and to address the needs of Stakeholders. Responsibilities: Ability to quickly learn, understand, and work with new emerging technologies, and methodologies A drive to work on financial data systems & pipelines including experience working with large datasets Work effectively with other engineers, product managers in a collaborative environment Involve in identifying data quality issues and apply techniques to improve the same Understand the requirements of stakeholders to build optimal solutions for various data challenges Qualifications: Pursuing bachelor's or master's degree in computer science or in a related field Understanding of data engineering and data modeling Strong analytical, comprehension and problem-solving skills 1+ years of academic or industry experience with Python or Java, or Scala 1+ years of experience with SQL Experience with Agile and SDLC, Git workflows, and CI/CD Preferred Skills: Experience with cloud services like AWS and/or GCP Knowledge of RESTful services and APIs Experience building data pipelines using Spark/Airflow Exposure to pub-sub messaging systems like Kafka Be a Part of Trepp’s Summer Internship ProgramTrepp’s Summer Internship is a robust program that allows students to gain exposure to our unique business and industry. By being a member of our growing organization, interns will work alongside our talented and committed professionals who will help them build a strong foundation for achieving career goals.Our interns get the opportunity to grow their network and enhance their skill set; leverage what they've learned in the classroom, while also being challenged with meaningful projects that drive impactful changes to our business. From day one, our interns receive formal and informal training to help them excel in their new roles. Our inclusive and collaborative environment encourages them to bring their unique and diverse perspectives to the table.SalaryHourly rate starting from $20 (commensurate with experience) plus over-time eligible.Workplace PolicyNYC, PA, and London office-based positions: Trepp’s offices follows a 3-2 hybrid-working policy with the expectation of in-office work on Tuesday-Thursday and the option to work from home on Monday and Friday.Trepp, Inc. is an equal opportunity / affirmative action employer, complying with all laws governing employment in each jurisdiction in which operating, and provides equal opportunity to all applicants and employees. All qualified applicants will be considered without regard to race, color, religion, gender, national origin, age, disability, marital or protected veteran status, sexual orientation, gender identity and other status protected by applicable laws. Show more Show less Seniority level Internship Employment type Internship Job function Marketing Industries Financial Services and Information Services","['Kafka', 'Airflow']",2024-04-21 15:53:59.643028
Tapcheck,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-tapcheck-3903721674?position=10&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=WSC4j6%2FG5RdrdOh236V1iw%3D%3D&trk=public_jobs_jserp-result_search-card,"Tapcheck is looking for a Data Engineer to join our growing Data team. In this role you will partner closely with stakeholders to gather requirements and play a crucial role in shaping the future of Tapcheck's data infrastructure.What You’ll Do:Collaborate with business stakeholders to gather and translate their requirements into actionable data solutionsDesign, develop, and maintain data pipelines using Azure Data Factory, with SQL queries as the primary transformation stepsImplement data quality checks to ensure data accuracy and consistencyPerform data analysis and reconciliation, supporting business users in their reporting needsUtilize Omni Analytics, our new BI tool, to build insightful reports and dashboardsContribute to the modern data stack and drive the migration towards a formal data warehouseWhat You’ll Bring:3+ years of experience as a BI / data engineer, preferably in a smaller company or start-up environmentStrong attention to detail and ability to work with complex data setsProficiency in SQL, including the mastery of subqueries, CTEs, and window functionsExperience with data pipeline and ETL tools (Azure Data Factory, DBT, Fivetran)Familiarity with reporting and visualization tools such as Looker and Omni AnalyticsKnowledge of modern data warehousing concepts and dimensional modelingExcellent communication and interpersonal skillsCompensation: $105,000 - $115,000. The actual base salary will depend on numerous factors such as: location, experience, training, knowledge. and skills. Tapcheck reserves the right to amend, change, alter, and revise pay ranges and benefits offerings at any time. All applicants acknowledge that by applying to this position you understand that this specific pay range is contingent upon meeting the qualifications and requirements of the role, and for the successful completion of the interview selection and process. It is at the Company's discretion to determine what pay is provided to a candidate within the range associated with the role.About Tapcheck:Tapcheck is a digital platform offering an easy and convenient way to access on-demand earnings early. Available at no cost to employers, our app-based on-demand pay solution helps relieve the financial stress that many employees experience on a daily basis.The Tapcheck team is passionate about our mission to improve financial wellness and boost business productivity. By giving workers the ability to transfer wages they’ve earned directly to their bank account or pay card without waiting for payday, Tapcheck eliminates the need for high-interest payday loans or employer-funded cash advances.How We Get Things Done:Our core values act as a steadfast guide, directing our decisions and anchoring our actions. We consider these values non-negotiable, especially when it comes to our hiring process. Humility: We believe in the power of humility. We value team players who are down-to-earth, respectful, and open to learning from others. Our employees approach challenges with a positive attitude, acknowledging their strengths and weaknesses while celebrating the achievements of their colleagues Grit: We admire individuals with grit – those who demonstrate unwavering determination and resilience in the face of obstacles. At Tapcheck, we take pride in overcoming challenges together, pushing the boundaries of what is possible, and embracing failure as an opportunity for growth Raising the Bar: Continuous improvement is at the heart of our culture. We are committed to setting high standards and pushing ourselves to exceed them. We seek employees who are innovative and strive for excellence, constantly seeking ways to enhance our products, services, and processes Striving for Growth: We foster an environment that encourages personal and professional development. Our employees are driven to learn, grow, and adapt to new circumstances. We support individuals who take initiative, seek out new challenges, and actively contribute to their own growth and the growth of the companyWhy Join Tapcheck?Competitive basePaid Time OffHealth InsuranceDental InsuranceVision Insurance401K MatchEqual Employment Opportunity PolicyTapcheck, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Show more Show less Seniority level Entry level Employment type Full-time Job function Engineering Industries Technology, Information and Internet","['dbt', 'Fivetran', 'Looker']",2024-04-21 15:53:59.643028
Jada Systems Inc,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-jada-systems-inc-3878754336?position=11&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=CeBspQtkynzbBXiATq2Fyw%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Porter,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-porter-3903243084?position=12&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=N0S21JzNueSOHIN1nlL0cg%3D%3D&trk=public_jobs_jserp-result_search-card,"Who We ArePorter combines the power of analytics with the power of care. Porter is a leading healthcare IT and services platform for care and coverage coordination that optimizes outcomes and member experience. We deliver understanding, compassion, information, and peace of mind for your members. Driven by robust AI analytics, Porter’s Care Guide team helps the member navigate the healthcare delivery system, secures the right support for each member’s specific needs, and directs Porter’s team of expert clinicians to perform comprehensive in-home assessments, complete with lab and diagnostic testing. By coordinating the complexities of each unique care journey, Porter helps close the gaps with the largest impact on quality measures, total cost of care, risk adjustment, and member experience.What You'll DoIn this role, you'll be responsible for creating innovative web and mobile applications, ensuring our front end is responsive, efficient, and deployable across various public cloud regions. You'll apply your skills in developing both static and dynamic pages, integrating APIs with internal and external data sources, and managing sessions effectively.Responsibilities: Develop, maintain, and deliver technical documentation for ETL development, scheduled processes, frameworks, and transformations Implement data cleaning transformations using ETL tooling, scripting, and data science techniques Translate user stories into actionable tasks and participate in code reviews and design sessions within Agile processes Monitor and troubleshoot existing ETL processes, enhancing or creating new ones as needed Ensure the availability of data for reporting by monitoring and troubleshooting BI applications, datasets, frameworks, cubes, and reports Utilize Agile methodologies to deliver features, increase velocity, and facilitate team communication and cohesion Define and apply coding standards and best practices, promoting scalability, availability, and performance in technical designsWhat You'll NeedQualifications:Required: 5+ years of advanced SQL knowledge and experience with relational databases, including query authoring and familiarity with various database technologies Proficiency in database technology, including index strategies, performance tuning, optimization, and stored procedures Ability to perform root cause analysis on internal and external data and processes to address business questions and identify improvement opportunities Strong analytical skills for working with structured and unstructured datasets Extensive Python proficiency for advanced data manipulation, analysis, and visualization Hands-on experience implementing databases such as NoSQL (e.g., DynamoDB, MongoDB) and relational databases (e.g., PostgreSQL, - --MySQL, Microsoft SQL) Expertise in building data models and executing complex queries using SQL Experience in agile software methodologies Bachelor's degree or equivalent experience/certificationPreferred: Experience with development in public cloud platforms like AWS or Azure Highly valuable experience with Databricks, Spark, and big data technologies Preference for candidates with experience in machine learning, data science, and statistical modeling Healthcare experience is highly regardedWhat We Offer Competitive Salary Generous Stock Options Medical, Dental, and Vision (benefits within 30 days of hire) Fully Remote Paid vacation and holidays A fun team and special culture Show more Show less Seniority level Not Applicable Employment type Full-time Job function Information Technology Industries Software Development","['PostgreSQL', 'Databricks', 'MongoDB', 'MySQL']",2024-04-21 15:53:59.643028
LatentView Analytics,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-latentview-analytics-3901392067?position=13&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=%2Bh%2BVXPr37Rcal%2F0N%2FSSONg%3D%3D&trk=public_jobs_jserp-result_search-card,"Direct message the job poster from LatentView Analytics Mukthar Ahmed (He/Him) Mukthar Ahmed (He/Him) Assistant Manager - US Recruitment at LatentView Analytics Job Description:We are looking for a Product Data Engineer with advanced knowledge on SQL (Google SQL preferred), bigquery and Python. As a Data Engineer, you will play a crucial role in designing, building, and maintaining our data infrastructure to support various data initiatives and analytics needs within the organization.Job responsibility:Collaborate with stakeholders to understand business needs and translate them into scalable and reliable data systemsWork with data Engineering & Product Marketing teams to ensure right signals are being captured by understanding the product flowsBuilding data pipelines tracking site metrics ie., user journeys on conversion and re-acquisitionsDesign and develop data pipelines with strong focus on data utility for analytical teamsResponsible for documentation of the infrastructure design & workflowsA continuous drive to explore, improve, enhance, automate, and optimize systems and tools to best meet evolving business and market needs. Attention to detail, coupled with the ability to think abstractlyStrong problem solving and quantitative skillsGood communication and stakeholder managementPrimary Skills:Excellent SQL skills (Google SQL preferred), hands on experience with dashboard tools like any of Looker / plx/ Tableau. Secondary Skills:Navigate through data ambiguity in identifying right signals for data, Excellent communication skills and ability to seamlessly work with XFN stakeholder teams independentlyAdditional Skills:Prior experience in building data pipelines for tracking site metrics ie., user journeys on conversion and re-acquisitions. Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Consulting, Engineering, and Information Technology Industries IT Services and IT Consulting","['BigQuery', 'Tableau', 'Looker']",2024-04-21 15:53:59.643028
"eTek IT Services, Inc.",Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-etek-it-services-inc-3906016196?position=14&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=yDs%2B5S6asvnN8MwRyhZNzw%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Edmunds,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-edmunds-3886782670?position=15&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=e8%2FsIqNKmZIcDMBy3vKrIQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Edmunds offers flexibility to work fully remote, from our Edquarters, or a combination of bothAt Edmunds we’re driven to make car buying easier. Ever since we began publishing printed car guides in the 60’s, the company has been in the business of trust, innovating ways to empower and support car shoppers. When Edmunds launched the car industry’s first Internet site in 1994, we established a leadership position online and have never looked back. Now, as one of the most trusted review sites on the Internet, millions of visitors use our research, shopping and buying tools every month to make an easy and informed decision on their next car. For consumers, we bring peace of mind. For dealers, we make tools to help them solve their problems and sell more cars. How do we do it, you ask? The key ingredients are our enthusiastic employees, progressive company culture and cutting-edge technology. Want to join the team? Read on to find out how!What You’re Applying ForEdmunds is looking for a Data Engineer to help us manage the data explosion dilemma! You will be joining a strong, determined and results-oriented Data Engineering team that is transforming Edmunds from IT to real time data-driven. You will get hands-on experience solving complex data problems using Big Data frameworks and methodologies. You’ll be helping the company make real time decisions based on the torrent of data it receives, empowering analytics on existing products as well as providing insights on potential new opportunities for growth.What You’ll DoCreate and maintain scalable, maintainable and reliable data pipelines that process very large quantities of structured and unstructured data in both batch and real time.Enhance and maintain the data lakehouse that powers the core of the company’s decision making process.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Collaborate with team members with a goal of improving personal knowledge of the systems, ensuring that code changes meet business goals and technology best practices.Regularly interact and collaborate with colleagues across functions and teams.Work with stakeholders including the Executive, Product, and Data teams to assist with data-related technical issues and support their data infrastructure needs. What You NeedExcellent problem solving, troubleshooting, and communication skills especially in a hybrid and remote environment.Desire to learn new technologies.Demonstrated ability to design and write maintainable software.Understanding of software engineering best practices, object oriented analysis & design, and design patterns & algorithms.Experience enhancing and evolving existing systems.Almost all of our codebase is in Scala and Python, but we only require that you have a high proficiency in at least one object oriented or functional programming language.A strong candidate will also have:Experience writing ETL Jobs and working with data at scale.Experience writing and maintaining real time / streaming data pipelines.Familiarity with some of the following: Spark, Scala, Python, AWS, Databricks, Airflow.Fluency in SQLOther nice to haves: Kubernetes, Machine Learning.The compensation range for this position is $109,900 - $191,815 per year. The base pay will take into account internal equity as well as job-related knowledge, skills, and experience among other factors. In addition, Edmunds offers full-time employees a comprehensive total rewards package including the benefits listed below.Edmunds PerksFlexible time off13 Paid HolidaysComprehensive Health Benefits (medical, dental, vision, life and disability)Flexible Spending Accounts (Employees) and Health Savings Accounts (Employee and Employer Contributions)401K Plan with company matching at 100%, up to 6% of eligible salary with immediate vestingStock purchase programCarMax vehicle discountUp to 4 months Paid Parental LeaveHeartCash matches employee donations to the causes that are important to them2 Days of Paid Time Off for time to dedicate to social impact causesFitCash covers a portion of gym or fitness activity feesWell being sessions and events such as yoga, meditation and walking challengesOn-going career development sessions and an annual learning eventPet insuranceSabbatical leaveEducation ReimbursementPre-tax spending accounts for qualified transportation expensesPlus a coffee bar, frozen yogurt and more!Working @ Edmunds.com:Employees think it’s a pretty great place to work and some pretty impressive publications think it is too: we have been recognized as one of the best places to work by the Fortune Magazine and Great Places to Work, LA Business Journal (for the last 6 years!), Computerworld, Built in LA and Inc. Magazine. We've also been identified as one of the best workplaces specifically in Technology and also for Diversity and Asian Americans. If you’re interested in learning more and joining our mission, we’d love to hear from you!Edmunds will consider for employment qualified candidates with criminal histories in a manner consistent with the requirements of all applicable laws. Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Motor Vehicle Manufacturing","['Databricks', 'Airflow']",2024-04-21 15:53:59.643028
LinkedIn,Staff Data Engineer - Data Science,https://www.linkedin.com/jobs/view/staff-data-engineer-data-science-at-linkedin-3873198031?position=16&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=wU4o%2B1GbM5BXaOOvP2mmrg%3D%3D&trk=public_jobs_jserp-result_search-card,"Direct message the job poster from LinkedIn Becca Slattery, PHR Becca Slattery, PHR AI/Machine Learning and Data Science Talent Acquisition Senior Manager @ LinkedIn | Technical Recruiting LinkedIn is the world’s largest professional network, built to help members of all backgrounds and experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters.This role will be based in Sunnyvale.At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together.LinkedIn’s Data Science team leverages big data to empower business decisions and deliver data-driven insights, metrics, and tools in order to drive member engagement, business growth, and monetization efforts. With over 900 million members around the world, a focus on great user experience, and a mix of B2B and B2C programs, LinkedIn offers countless ways for an ambitious data engineer to have an impact and transform your career.We are now looking for a talented and driven individual to accelerate our efforts and be a major part of our data-centric culture. This person will work closely with various cross-functional teams such as product, marketing, sales, engineering, and operations to develop infrastructure and deliver tools or data structures that enable data-driven decision-making. Successful candidates will exhibit technical acumen and business savviness with a passion for making an impact by enabling both producers and consumers of data insight to work smarter.Responsibilities:Work with a team of high-performing data science professionals, and cross-functional teams to identify business opportunities and build scalable data solutions.Build data expertise, act like an owner for the company and manage complex data systems for a product or a group of products.Perform all of the necessary data transformations to serve products that empower data-driven decision making.Establish efficient design and programming patterns for engineers as well as for non-technical partners.Design, implement, integrate and document performant systems or components for data flows or applications that power analysis at a massive scale.Ensure best practices and standards in our data ecosystem are shared across teams.Understand the analytical objectives to make logical recommendations and drive informed actions.Engage with internal data platform teams to prototype and validate tools developed in-house to derive insight from very large datasets or automate complex algorithms.Contribute to engineering innovations that fuel LinkedIn’s vision and mission.Basic Qualifications:Bachelor's Degree in a quantitative discipline: Computer Science, Statistics, Operations Research, Informatics, Engineering, Applied Mathematics, Economics, etc4+ years of relevant industry or relevant academia experience working with large amounts of dataExperience with SQL/Relational databasesBackground in at least one programming languages (e.g., R, Python, Java, Scala, PHP, JavaScript)Preferred Qualifications:BS and 7+ years of relevant work experience, MS and 5+ years of relevant work experience, or Ph.D. and 3+ years of relevant work/academia experience working with large amounts of dataMS or PhD in a quantitative discipline: Statistics, Operations Research, Computer Science, Informatics, Engineering, Applied Mathematics, Economics, etc.Suggested Skills :JavaDistributed SystemsRelational DatabasesTechnical LeadershipYou will Benefit from our Culture:We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levelsLinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $144,000.00 to $235,000.00 Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits.Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice Show more Show less Seniority level Not Applicable Employment type Full-time Job function Engineering Industries Technology, Information and Internet and IT Services and IT Consulting",[],2024-04-21 15:53:59.643028
Microsoft,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-microsoft-3902366180?position=17&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=FYsYX634kAQGCn3xdLNtZg%3D%3D&trk=public_jobs_jserp-result_search-card,"The Azure Core Organization is responsible for creating the foundation of Microsoft’s Cloud Platform for utility computing. This platform is one of the lowest levels of the services software/hardware stack and includes an efficient, virtualized computational substrate, a fully automated service management system, and a comprehensive set of highly scalable storage services.The Azure Compute Capacity and Efficiency (AC2E) team is the team in Azure Core tasked with managing all aspects of Compute capacity and efficiency management across the fleet. Capacity Management needs to ensure that on the one hand, there is sufficient capacity across all regions, allocation domains, and hardware infrastructure to meet all customer demand, while on the other hand ensuring that capacity is provisioned efficiently thereby avoiding overspending and COGS/CAPEX impact. At the scale of Azure’s business, managing this trade-off across the entire Azure Compute fleet is an enormously complex task, where improvements can make the difference between customer allocation failures on the one hand, and gargantuan savings on the other.As a Data Engineer in the team, you will work closely with our software engineers, program managers, and data scientists across different teams within Azure Core. You will also collaborate with a variety of internal partner teams across Azure and Microsoft. You will build reliable, secured, highly scalable, performant data pipelines to enhance the Azure Compute allocation, deliver capacity management and efficiency improvements. The value of your work will be reflected in improvements to the Azure platform, Azure service capacity fulfillment rate, customer satisfaction, and various efficiency metrics, including COGS reduction. You will have opportunities for mentorship, accelerate your career growth, and work on truly high-business impact areas.Microsoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond.In alignment with our Microsoft values, we are committed to cultivating an inclusive work environment for all employees to positively impact our culture every day.ResponsibilitiesData Engineering, insights generation and analytics with deep domain knowledge understanding.With detailed instructions, you’ll implement code to extract raw data, validate its quality, and ensure the correct data is ingested within your area of work after cooking, data transformation.You will follow data modeling and handling procedures to maintain compliance with all applicable laws and policies across your assigned workstreams.Integrating analytics intelligence into the Azure platformAdvocate for best practices in data engineering.OtherEmbody our Culture and ValuesQualificationsRequired Qualifications: Bachelor's Degree in Computer Science, or related field OR equivalent experience.Other RequirementsAbility to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.Additional / Preferred QualificationsBachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering , or related field AND 1+ year(s) experience in business analytics, data science, data modeling or data engineering workOR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related fieldOR equivalent experience.Experience in business analytics, data science, software development, data modeling OR data engineering work.Data Engineering IC2 - The typical base pay range for this role across the U.S. is USD $76,400 - $151,800 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $100,300 - $165,400 per year.Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here:https://careers.microsoft.com/us/en/us-corporate-payMicrosoft will accept applications and processes offers for these roles on an ongoing basis.#AzurecorejobsMicrosoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations. Show more Show less Seniority level Not Applicable Employment type Full-time Job function Information Technology Industries Software Development",[],2024-04-21 15:53:59.643028
TOCA Football,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-toca-football-3892891287?position=18&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=WF%2Fvy80gzaHr8dciiOgoyw%3D%3D&trk=public_jobs_jserp-result_search-card,"At TOCA, we are passionate about people and the power of sport. We believe in creating an environment that becomes the “third home” for our guests — where they learn, where they live, and where TOCA becomes the place where they play. Whether they’re kicking a soccer ball for the first time, focused on finding their best, or rediscovering their passion for the game, we are here to support and guide them every step along the way. Everyone deserves the opportunity to experience the joy and fulfillment that sports can bring, regardless of background and skill levels. Our ultimate goal is to create a consistent and amazing experience for everyone who interacts with TOCA, whether it is our dedicated team members or esteemed guests.What makes a TOCA Teammate? An individual that seeks to...Play HardCare DeeplyGrow TogetherStrive for ExcellenceCreate Awesome ExperiencesJob Highlights:Reports To: Director, Data & AnalyticsLocation: Remote - Atlanta, GACompensation: Competitive salary with bonus opportunitiesBenefits: Health, Vision, Dental, 401K, and Paid Time OffPosition Overview:We are seeking an experienced and dynamic Data Engineer to support our data infrastructure within TOCA. This position plays a critical role in enabling data-driven decision-making and driving business growth through the utilization of data assets. A successful candidate in this role will be responsible for maintaining and improving TOCA’s data infrastructure, evolving the CI/CD strategy, and collaborating internally on the continued expansion of data pipelines.What will you be doing?Guide and implement architectural improvements to our data infrastructureMaintain our data pipelines, production systems, and orchestration processesEvolve and guide our CI/CD strategy and best practicesCollaborate with the team on data modeling and contribute to building out new modelsAdvise on best practices for performance optimization and data designDevelop new data pipelinesPerform system analysis and monitoring of pipelines to identify areas of improvementWhat will you bring to the TOCA team? Understanding and experience building ELT pipelines and with data orchestration tools.Familiarity with CI/CD strategy and best practices.Proficient at writing performant SQL and reviewing peers SQL.Strong knowledge of data warehouses and business intelligence tools.Clear and direct communication skills about complex, technical topics.A data curious mindset - you instinctively seek new and existing data, question it, explore it, and know how to communicate your findings.Strong time management and organizational skills.Strong written communication skills for documenting work for technical and non-technical audiences.Ability to be able to work with autonomously and on multiple projects simultaneously.Comfort working with a fully remote, globally distributed company.Nice to Have:Prior experience or the drive to work for a fast-paced and growing company is an added advantageExperience working with our data stack: Snowflake, DBT, Sigma, Airflow, Monte Carlo, Github, Datafold, FivetranExperience working with other technologies: Hubspot, Google AnalyticsWhat Does Success Look Like?30 Days: Comfortability working with our data and our data stack60 Days: Contributing to the expansion of the data warehouse and begin working on first new data pipeline90 Days: Rolling out first new data pipeline, supporting team members with new data sources, and begin adding efficiencies to workflowsWhat's Next?Submit your application today to start the journey of joining the TOCA squad!TOCA Football, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Show more Show less Seniority level Entry level Employment type Full-time Job function Information Technology Industries Spectator Sports","['Snowflake', 'Airflow', 'dbt', 'Fivetran']",2024-04-21 15:53:59.643028
Meta,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-meta-3882999613?position=19&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=NG8yt2J%2B6bhJRYuJcg%2B0mg%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
STARK BANK,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-stark-bank-3869712607?position=20&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=qo6NZSvUrGQkd7xDyVkDUw%3D%3D&trk=public_jobs_jserp-result_search-card,"Company Intro:Our world is immersive and visceral. Content today is static and boring. We are Momenti, video that you interact with, bringing visceral experiences to all content. Build deeper connections and emotions with multi-dimensional video, and bring life to content by breaking the 4th wall. Join the content revolution! The world is alive, and so should your captured moments. Momenti is here.Momenti is a startup with technologies for creating and playing Gesture Interactive Video (GIV) that responds to user gestures. We are developing service software that enables content creation, editing, and playback based on our unique media model and providing these services to users worldwide. We are also developing data analysis technologies to analyze and visualize user gestures to improve the quality and service of media content (you can experience the technology demo on our website https://momenti.tv ).Job summary:Collect, analyze, and visualize gesture interaction data generated by users worldwide in real time and derive insights to improve the content usage experience based on this data. We are building a data pipeline, operating data storage, and improving query efficiency to convert data into value and deliver it with Momenti. We are looking for engineers who can participate with us.Job Responsibilities:Building and operating a data analysis pipelineBuilding and operating a data pipeline for monitoring systems for managing indicators (business, product)Working based on Cloud Pubsub for streamingAnalyzing user data and deriving business insightsFunnel Analysis, Heatmap Analysis, etc.MLOps, developing and operating machine learning modelsTech Stack Requirements:GCP, Cloud PubSub, BigQuery Cloud SQL (PostgreSQL), Cloud Storage, DBT, Metabase, GrafanaQualifications:Bachelor's degree in computer science, software engineering, information technology, or a related fieldExperience in building and operating a data pipeline on a cloud platformProgramming experience for automation (Python, Go, etc.)Experience using data analysis and data SaaS (Amplitude, Mixpanel, etc.)Preferred QualificationsExperience in data analysis based on machine learningExperience building advertising dashboards/back officesExperience in ML model serving/MLOps.Salary & Benefits: Full-time Position Salary range is $105,000 - 160,000 yr depending on experience Paid Time Off & Holiday Health, Dental, and Vision Insurance Show more Show less Seniority level Not Applicable Employment type Full-time Job function Information Technology Industries Banking","['PostgreSQL', 'BigQuery', 'PubSub', 'dbt']",2024-04-21 15:53:59.643028
Meta,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-meta-3882997984?position=21&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=5xDb7C%2F0ZQNrxQTq3oHPoQ%3D%3D&trk=public_jobs_jserp-result_search-card,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.Data Engineer Responsibilities:Partner with leadership, engineers, program managers and data scientists to understand data needsApply proven expertise and build high-performance scalable data warehousesDesign, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)Securely source external data from numerous partnersIntelligently design data models for optimal storage and retrievalDeploy inclusive data quality checks to ensure high quality of dataOptimize existing pipelines and maintain of all domain-related data pipelinesOwnership of the end-to-end data engineering component of the solutionSupport on-call shift as needed to support the teamDesign and develop new systems in partnership with software engineers to enable quick and easy consumption of dataMinimum Qualifications:BS/MS in Computer Science or a related technical field5+ years of Python or other modern programming language development experience5+ years of SQL and relational databases experience5+ years experience in custom ETL design, implementation and maintenance3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)3+ years experience with Data ModelingExperience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)2+ years experience working with enterprise DE tools and experience learning in-house DE toolsBachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.Preferred Qualifications:Experience with more than one coding languageExperience designing and implementing real-time pipelinesExperience with data quality and validationExperience with SQL performance tuning and end-to-end process optimizationExperience with anomaly/outlier detectionExperience with notebook-based Data Science workflowExperience with AirflowExperience querying massive datasets using Spark, Presto, Hive, Impala, etc.Experience building systems integrations, tooling interfaces, implementing integrations for ERP systems (Oracle, SAP, Saleforce etc).About Meta:Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today—beyond the constraints of screens, the limits of distance, and even the rules of physics.Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com. Show more Show less Seniority level Not Applicable Employment type Full-time Job function Information Technology Industries Technology, Information and Internet","['Redshift', 'BigQuery', 'Airflow', 'presto']",2024-04-21 15:53:59.643028
Vuori,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-vuori-3855584436?position=22&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=znjmk6hv4feAeOrwLvL2Hg%3D%3D&trk=public_jobs_jserp-result_search-card,"Company DescriptionVuori is re-defining what athletic apparel looks like: built to move and sweat in but designed with a casual aesthetic to transition into everyday life. We draw inspiration from an active coastal California lifestyle; an integration of fitness, creative expression and life. Our high energy fast paced office environment is reflected in the clothes we make. We aim to inspire others to take on all aspects of their lives with clarity, enthusiasm and purpose…while having a lot of fun along the way. We are proud to be an outlet for opportunity and for personal growth and success.Job DescriptionResponsibilities include but are not limited to:Data Pipeline Development:Design, develop, and maintain scalable and efficient data pipelines. Extract, transform, and load (ETL) data from various sources into our data warehouse. Enable data quality and integrity throughout the ETL process. Data Architecture:Collaborate with cross functional tech leads and architects to design and optimize data models and database structures. Implement best practices for end to end data pipe management on data lake. Work on data warehousing solutions, such as Azure ADF, Snowflake etc. Data Integration:Integrate third-party data sources and APIs to enrich our datasets. Enable processes for monitoring, exception management across end to end data pipe build to ensure integrity and reliability of data engineering solutions Implement data connectors and data ingestion processes Work on designing and defining new ways of data integrations while managing existing data integrations Performance Optimization:Monitor and optimize data pipelines and query performance. Troubleshoot and resolve data-related issues in a timely manner. Data Security and Compliance:Ensure data security and compliance with relevant data protection regulations (e.g., GDPR, HIPAA). Implement access controls and encryption mechanisms. Collaboration:Collaborate with analytics, product leads and business product owners to define and build best in class data ecosystem driving business analytic capabilities Be part of agile operating model alongside analytics and business teams to drive collective data & analytics capabilities Work alongside planning, master data and other teams looking for clean, connected data and provision datasets as API’s or onetime per need Support data consumers by providing access to clean and well-organized datasets. Documentation:Maintain documentation for data pipelines, schemas, and data dictionaries. Document end to end data pipes and ongoing enhancements to them Create and update documentation on data engineering processes and standards. QualificationsBachelor's degree in Computer Science, Information Technology, or a related field. Master's degree preferred. 5 years of experience as a Data Engineer or similar role. Proficiency in data modeling, ETL development, and data warehousing. Strong programming skills in languages like Python, Java, or Scala. Experience with data pipeline orchestration tools Knowledge of SQL and proficiency in working with relational and NoSQL databases. Familiarity with cloud platforms (e.g., Azure, Snowflake) and associated data services. Excellent problem-solving and communication skills. Ability to work independently and as part of a cross-functional team. Any specific certifications or additional qualifications preferred.If you are a highly motivated and detail-oriented Data Engineer with a passion for working with data, we encourage you to apply and join our innovative team at Vuori. Help us drive data-driven decisions and make a meaningful impact in athletic performance apparel business. Additional InformationPay Range: $100,000-155,000/yrBenefits:Health InsurancePaid Time OffEmployee Discount401(k)All your information will be kept confidential according to EEO guidelines. Show more Show less Seniority level Associate Employment type Full-time Job function Other Industries Retail Apparel and Fashion",['Snowflake'],2024-04-21 15:53:59.643028
Tech Mahindra,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-tech-mahindra-3901937427?position=23&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=W6lwS48N89byIaQKOQSg5w%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for Strong Data Engineer with 6+ year’s experience.Required Skills: ADF pipelines, SQL, Kusto, Power BI, Cosmos (Scope Scripts). Power Bi, ADX (Kusto), ADF, ADO, Python/C#.Good to have – Azure anomaly Alerting, App Insights, Azure Functions, Azure FabricQualifications for the role 5+ years experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Specific experience working with COSMOS and Scope is required for this role. Experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases is a plus. Experience with investigating and on-boarding new data sources in a big-data environment, including forming relationships with data engineers cross-functionally to permission, mine and reformat new data sets. Strong analytic skills related to working with unstructured data sets. A successful history of manipulating, processing and extracting value from large disconnected datasets. Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Engineering, Information Technology, and Other Industries IT Services and IT Consulting and Engineering Services",[],2024-04-21 15:53:59.643028
Atlantic Group,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-atlantic-group-3854494544?position=24&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=%2FSxYmzi1EbXCxaOe7sJKWg%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
JDE Peet's,Business Intelligence Data Engineer,https://www.linkedin.com/jobs/view/business-intelligence-data-engineer-at-jde-peet-s-3871935665?position=25&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=skM%2F55653QoZc8Uv3TcmrA%3D%3D&trk=public_jobs_jserp-result_search-card,"Company DescriptionJDE USA, the global luxury tea and coffee brand is seeking a Business Intelligence Data Engineer. As a BI Data Engineer you will be responsible to ensure business partners at JDE USA can access the data they need to make informed decisions seamlessly. This role will contribute toward developing standard reporting and dashboards that will effectively scale with our growing business. This role reports to the Director of IT.Job DescriptionCollaborate with business stakeholders and system owners to gather requirements for new development projects related to opportunities for reporting solutions.Develop and maintain data flows into the JDE corporate data lake for ad hoc reporting needs.Develop and maintain Power BI dashboards to support our internal business community and promote data driven decision making.Lead data governance projects to ensure clean, consistent categorization of customers, products, and orders.Monitor, troubleshoot and provide day-to-day support to our business team members to ensure maximum value from our corporate data infrastructure.Coordinate and perform quality unit testing and coordinate end user testing for all new development.Teach and train corporate partners on how to effectively use corporate tools to extract meaningful insights.QualificationsProven experience as a business intelligence analyst or data engineer in a manufacturing environment. Experience in the Consumer-Packaged Goods (CPG) industry is a plus.Minimum of 3 years of hands-on experience with Microsoft SQL development in a business environment, Microsoft Azure certified. Familiarity with data management and database administration concepts, principles, and processes.Minimum of 3 years of experience as a business analyst or similar role supporting the development of APIs, EDI/integration technologies.Proven expertise in data visualization, specifically using Power BI or Tableau.Strong attention to detail and demonstrated capabilities in data governance and agile development principles.Ability to communicate technical concepts to non-technical audiences.Excellent analytical, mathematical, and creative problem-solving skills.Ability to work independently and manage multiple priorities in a fast-paced environmentThe ideal candidate will be able to work collaboratively with cross-functional teams and possess excellent communication and interpersonal skills. If you have a passion for building software solutions, are committed to accuracy and attention to detail, and thrive in a fast-paced, dynamic environment we encourage you to apply for this exciting opportunity with JDE USA. Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Other Industries Consumer Goods",['Tableau'],2024-04-21 15:53:59.643028
Parallel Consulting,Junior Data Engineer,https://www.linkedin.com/jobs/view/junior-data-engineer-at-parallel-consulting-3868478683?position=26&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=qYnAllpzK%2BPCg%2BgL9d8j%2Bg%3D%3D&trk=public_jobs_jserp-result_search-card,"Junior Data EngineerThe FirmWe are working with the Director of Data Science of a boutique management consulting firm that is continuing to grow its Analytics and Engineering team. This is a great opportunity to join a firm that uses economics, statistics, and data science to solve problems for our federal and private-sector clients.As Junior Data Engineer, you would be joining a cross-functional team of data scientists, other engineers, and business analysts, and be responsible for supporting the build of data integration solutions in a modern cloud environment that help their clients analyze data more efficiently.RequirementsBachelor's Degree and at least 2 years of experience in programming in Python and SQLExperience with database designs, implementing and managing data pipelines within a cloud environment (AWS/Azure preferred)Skills in data modeling, statistical modeling, and/or data visualizations would be valuableWe do require US Citizens for this role that have an ability to obtain a public trust clearance.Location: DC/NoVA – Flexible work environment (1-2 days per week in office)Comp: $90-100k Base + Bonus Show more Show less Seniority level Associate Employment type Full-time Job function Information Technology Industries Business Consulting and Services",[],2024-04-21 15:53:59.643028
"eTek IT Services, Inc.",Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-etek-it-services-inc-3900983042?position=27&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=r%2BAm1fwaQc4GdLPdU6%2FMXg%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Amazon,"Data Engineer, Amazon",https://www.linkedin.com/jobs/view/data-engineer-amazon-at-amazon-3880458352?position=28&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=f7I0Iwo5ukH%2FzmvbwvipVg%3D%3D&trk=public_jobs_jserp-result_search-card,"DescriptionWant to be part of an exciting journey of building the Next Generation Financial Systems Architecture to enable Amazon businesses across Retail, Digital, AWS, Physical stores, Corporate businesses, Amazon subsidiaries and many more?Want to shift your experiences from working at the peripheries of the systems to actually start owning and designing their cores?The Finance Automation team is looking for a Data Engineer to build our next generation of data engineering services in the Amazon finance space. The ideal candidate will be passionate about building large, scalable, and fast distributed systems on the AWS tech stack. The candidate will be part of a team with a goal to innovate and impact the entire finance business space within Amazon. We are one of the fastest growing Data Engineering teams across Amazon with technology orientation.We are looking for a candidate with background in the AWS technology stack (S3, Glue, IAM, Redshift) and data warehousing experience with interest in data mining and ability to identify emerging patterns and trends from large amounts of data. The candidate is expected to have strong experience with all standard data warehousing technical components (e.g. ETL, Reporting, and Data Modelling), infrastructure (hardware and software) and its integration. The candidate has experience in dimensional modelling, excellent problem-solving ability, capability in dealing with huge volumes of data, and a short learning curve. Written and verbal communication skills are required as the candidate will work very closely with different teams and senior leadership across Amazon.Along with complex problems to solve, we provide a world class work environment side by side with talented team members in the data engineering space and the opportunity to contribute and create history while having fun.Key job responsibilitiesDesign, build and own components of a high volume data warehouse.Build efficient data models using industry best practices and metadata for ad hoc and pre-built reporting.Interface with business customers, gathering requirements and delivering complete data and reporting solutions owning the design, development, and maintenance of ongoing metrics, reports, analyses, and dashboards to drive key business decisions.Continually improve ongoing reporting and analysis processes, automating and simplifying self-service support capabilities for our customers.Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources.Own the functional and non-functional scaling of software systems in your area.Provide input and recommendations on technical issues to other engineers, business stake holders, and data analysts.Collaborate with data scientists to continue to build and enhance new or existing ML programs.We are open to hiring candidates to work out of one of the following locations:Seattle, WA, USABasic Qualifications Experience as a data engineer or related specialty (e.g., software engineer, business intelligence engineer, data scientist) with a track record of manipulating, processing, and extracting value from large datasetsPreferred Qualifications Knowledge of batch and streaming data architectures like Kafka, Kinesis, Flink, Storm, BeamAmazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.Company - Amazon.com Services LLCJob ID: A2601916 Show more Show less Seniority level Not Applicable Employment type Full-time Job function Strategy/Planning, Analyst, and Information Technology Industries Software Development","['Redshift', 'Kafka', 'Kinesis']",2024-04-21 15:53:59.643028
Apexon,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-apexon-3844868840?position=29&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=r4nwuiVaMJtPZvn9M7W%2B5Q%3D%3D&trk=public_jobs_jserp-result_search-card,"Direct message the job poster from Apexon Mischelle Sharon Martis Mischelle Sharon Martis Talent Acquisition Manager at Apexon Job Overview We are looking for a Data Engineer to join our fast growing team. This person will be responsible for expanding and optimizing our current cloud based data pipeline architecture for the various cross functional teams. The ideal candidate has experience building robust data pipelines and reporting tools. However, someone with a strong application development background with keen interest in data engineering would be considered. The candidate will be collaborating extensively with Engineering, Analytics and Product teams to support functional use-cases and take data driven decisions. Who we are looking for Strong background in data processing & software engineering and can build high-quality, scalable data oriented products.Industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, EMR, etc..) for building efficient, large-scale data pipelines.Strong Software Engineering experience with in depth understanding of Python, Scala, Java or equivalentStrong understanding of data architecture, modeling and infrastructureExperience with building workflows (ETL pipelines)Experience with SQL and optimizing queriesProblem solver with attention to detail who can see complex problems in the data space through end to endWillingness to work in a fast paced environmentMS/BS in Computer Science or relevant industry experience.Strongly recommended (but optional) Experience building scalable applications on the Cloud (Amazon AWS, Google Cloud, etc..)Experience building stream-processing applications (Spark streaming, Apache-Flink, Kafka, etc..)Experience with Databricks and Snowflake Our Commitment to Diversity & Inclusion: Did you know that Apexon has been Certified™ by Great Place To Work®, the global authority on workplace culture, in each of the three regions in which it operates: USA (for the fourth time in 2023), India (seven consecutive certifications as of 2023), and the UK.Apexon is committed to being an equal opportunity employer and promoting diversity in the workplace. We take affirmative action to ensure equal employment opportunity for all qualified individuals. Apexon strictly prohibits discrimination and harassment of any kind and provides equal employment opportunities to employees and applicants without regard to gender, race, color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. You can read about our Job Applicant Privacy policy here Job Applicant Privacy Policy (apexon.com) Our Perks and Benefits: Our benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexer, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance. We also offer: o Health Insurance with Dental & Vision o 401K Plan o Life Insurance, STD & LTD o Paid Vacations & Holidays o Paid Parental Leave o FSA Dependent & Limited Purpose care Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Banking","['Snowflake', 'Databricks', 'Kafka', 'EMR']",2024-04-21 15:53:59.643028
Team Remotely Inc,Junior Data Engineer,https://www.linkedin.com/jobs/view/junior-data-engineer-at-team-remotely-inc-3907038188?position=30&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=D0kjFeGPllWpbUFTQ9UIPA%3D%3D&trk=public_jobs_jserp-result_search-card,"This is a remote position. Junior Data Engineer (1 year experience, remote)Be part of our future! This job posting builds our talent pool for potential future openings. We'll compare your skills and experience against both current and future needs. If there's a match, we'll contact you directly. No guarantee of immediate placement, and we only consider applications from US/Canada residents during the application process. Hiring Type: Full-Time Base Salary: $59K-$69K Per Annum.Position SummaryJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance the efficiency, reliability, and performance of CVS Health’s IT operations.Key Responsibilities include:Data pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. Data modeling: Create and maintain data models ensuring data quality, scalability, and efficiency Develop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency Data Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data Utilize big data technologies such as Kafka to process and analyze large volumes of data efficiently Implement data security measures to protect sensitive information and ensure compliance with data and privacy regulation Create/maintain documentation for data processes, data flows, and system configurations Performance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness Characteristics of this role:Team Player: Willing to teach, share knowledge, and work with others to make the team successful. Communication: Exceptional verbal, written, organizational, presentation, and communication skills. Creativity: Ability to take written and verbal requirements and come up with other innovative ideas. Attention to detail: Systematically and accurately research future solutions and current problems. Strong work ethic: The innate drive to do work extremely well. Passion: A drive to deliver better products and services than expected to customers. Required Qualifications2+ years of programming experience in languages such as Python, Java, SQL 2+ years of experience with ETL tools and database management (relational, non-relational) 2+ years of experience in data modeling techniques and tools to design efficient scalable data structures Skills in data quality assessment, data cleansing, and data validation Preferred QualificationsKnowledge of big data technologies and cloud platforms Experience with technologies like PySpark, Databricks, and Azure Synapse. EducationBachelor’s degree in Computer Science, Information Technology, or related field, or equivalent working experience Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Software Development","['Databricks', 'Kafka', 'NiFi', 'Synapse']",2024-04-21 15:53:59.643028
M Science,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-m-science-3876446098?position=31&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=FrOye0Le1%2BPL8p7Lp3lAKw%3D%3D&trk=public_jobs_jserp-result_search-card,"Direct message the job poster from M Science Colin Orr Colin Orr Lead Recruiter at M Science Data EngineerLocation: FlexibleJob Description:We are looking for a Data Engineer who will be working closely with data scientists and software engineers to build data pipelines that supports AI products using the latest technology. Your primary focus will be maintaining end to end data pipelines, including vector databases, that will be a foundation of AI/ML projects and products. Being part of a growing team with the latest technology, this person will be a player in shaping the future of data science ML / AI within the firm. The opportunities to grow and influence the growth of the company are tremendous.Responsibilities:Build and maintain an optimal, scalable data pipeline architecture to support our product features and AI initiativesWork with various large complex unstructured datasets that will be used in AI/ML productPrepare data for modeling and finetuningExplore ways to enhance data quality and reliabilityBuild monitoring dashboards for data pipelines, displaying key metrics for internal stakeholdersCollaborate with data scientists and software engineers on several projectsSkills & Qualifications:3+ years of work experience in data engineering or related software engineering roleProven experience in building and optimizing data pipelines using PySpark, Python and SQL in production environmentsExpertise in data pipelines, data visualization, and engineering in a cloud environment (Python, Spark, AWS, Airflow, Snowflake)Strong understanding of optimization techniques for large data pipelinesStrong problem-solving skills and attention to detailCurious and voracious appetite to learnExcellent communication and collaboration skills, with the ability to work effectively in cross-functional teamsExperience in MLOps or working on AI/ML related data pipelines is a plus Show more Show less Seniority level Associate Employment type Full-time Job function Engineering and Finance Industries Financial Services and Market Research","['Snowflake', 'Airflow']",2024-04-21 15:53:59.643028
Steneral Consulting,SQL Data Engineer,https://www.linkedin.com/jobs/view/sql-data-engineer-at-steneral-consulting-3848323021?position=32&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=QHuXFpAA%2BlX4rH8s%2BJvQMg%3D%3D&trk=public_jobs_jserp-result_search-card,"Must-have skills: 5+ y of exp with interpreting database tables and data to understand correct extraction and mapping; SQL and SQL queries; exp with PLSQL Developer, SQL Plus, SQL Navigator, and AWS Glue; Migration exp ; Salesforce, AWS Redshift, and Snowflake; columnar databases.RequirementsJob Description: Experience with interpreting database tables and data to understand correct extraction and mapping (5+ years); Experience as a Data Engineer, Database Developer, or similar role; Experience with Structured Query Language (SQL); Experience creating, modifying, and querying relational databases; Experience in the creation of SQL queries to retrieve specific data from Needles by specifying various search criteria; Experience formatting, running totals, summaries, custom formulas, and custom Functions (SQL Queries); Experience with PLSQL Developer, SQL Plus, SQL Navigator, and AWS Glue; Experience with database-backed web applications; Experience with SQL and database management systems (e.g., MySQL, PostgreSQL, MongoDB); Experience with data manipulation languages and the principles of database design; Experience in software development and user interface web applications; Bachelor’s or Master’s degree in Computer Science, Information Technology, or a related field.Preferred Experience with both structured and unstructured data; Experience with both SQL and NoSQL databases; Experience with columnar databases; Migration experience; Experience with Salesforce, AWS Redshift, and Snowflake.Responsibilities Include But Are Not Limited To The Following Develop, optimize, test, and create extracts, integrations, and migrations of data to new and existing databases to help users retrieve data effectively; Work with technologies in the data field to support advanced analytics and data processing capabilities. Show more Show less Seniority level Mid-Senior level Employment type Contract Job function Information Technology Industries Software Development","['PostgreSQL', 'Snowflake', 'Redshift', 'MongoDB', 'MySQL']",2024-04-21 15:53:59.643028
Porter,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-porter-3882488631?position=33&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=wl%2BKw9bdqeO9gkKnnuTA7w%3D%3D&trk=public_jobs_jserp-result_search-card,"Who We ArePorter seamlessly merges analytics with compassionate care to provide unparalleled healthcare coordination and optimization. Our platform empowers members with understanding, compassion, and peace of mind while leveraging robust AI analytics. Through our Care Guide team, we navigate members through the healthcare system, ensuring tailored support, comprehensive in-home assessments, and expert clinical services. By addressing care complexities, Porter enhances quality measures, reduces costs, and improves member experiences.Who You AreIn this role, you'll be responsible for creating innovative web and mobile applications, ensuring our front-end is responsive, efficient, and deployable across various public cloud regions. You'll apply your skills in developing both static and dynamic pages, integrating APIs with internal and external data sources, and managing sessions effectively.What You'll DoDevelop, maintain, and deliver technical documentation for ETL development, scheduled processes, frameworks, and transformations.Implement data cleaning transformations using ETL tooling, scripting, and data science techniques.Translate user stories into actionable tasks and participate in code reviews and design sessions within Agile processes.Monitor and troubleshoot existing ETL processes, enhancing or creating new ones as needed.Ensure the availability of data for reporting by monitoring and troubleshooting BI applications, datasets, frameworks, cubes, and reports.Utilize Agile methodologies to deliver features, increase velocity, and facilitate team communication and cohesion.Define and apply coding standards and best practices, promoting scalability, availability, and performance in technical designs.What You NeedRequired:5+ years of advanced SQL knowledge and experience with relational databases, including query authoring and familiarity with various database technologies.Proficiency in database technology, including index strategies, performance tuning, optimization, and stored procedures.Ability to perform root cause analysis on internal and external data and processes to address business questions and identify improvement opportunities.Strong analytical skills for working with structured and unstructured datasets.Extensive Python proficiency for advanced data manipulation, analysis, and visualization.Hands-on experience implementing databases such as NoSQL (e.g., DynamoDB, MongoDB) and relational databases (e.g., PostgreSQL, MySQL, Microsoft SQL).Expertise in building data models and executing complex queries using SQL.Experience in agile software methodologies.Bachelor's degree or equivalent experience/certification.Preferred:Experience with development in public cloud platforms like AWS or Azure.Highly valuable experience with Databricks, Spark, and big data technologies.Preference for candidates with experience in machine learning, data science, and statistical modeling.Healthcare experience is highly regarded.What We OfferCompetitive Salary Generous Stock Options Medical, Dental, and Vision (benefits within 30 days of hire)Fully Remote Paid vacation and holidaysA fun team and special culture Show more Show less Seniority level Associate Employment type Full-time Job function Engineering Industries Medical Practices, Hospitals and Health Care, and Veterinary Services","['PostgreSQL', 'Databricks', 'MongoDB', 'MySQL']",2024-04-21 15:53:59.643028
IntePros,Data Engineer II,https://www.linkedin.com/jobs/view/data-engineer-ii-at-intepros-3905333100?position=34&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=IdMiKDYq2lvHdY2EvZuoGQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Compensation Range:$45.00 - $53.00/hrWelcome to IntePros, a certified woman-owned company specializing in innovative and results-oriented recruiting and staffing solutions. We take immense pride in genuinely understanding what drives and inspires exceptional individuals like you. Your success is our priority, and we are dedicated to actively shaping your long-term career journey. At IntePros, we believe in comprehensive well-being. You have access to our medical, dental, vision, and mental health programs, ensuring your health and wellness are taken care of. To support your continuous growth, we also provide a $1,500 per year education and professional certification fund. Diversity and inclusion are cornerstones of our company ethos. IntePros is proud to be an equal opportunity employer. We do not discriminate in employment on the basis of race, color, religion, sex, pregnancy, gender identity, national origin, sexual orientation, disability, age, veteran or military status, retaliation, or any other characteristic protected by law. We celebrate the rich tapestry of backgrounds and perspectives that make us stronger as a team. Please note that only qualified individuals being considered will be contacted. We appreciate your interest and look forward to potentially embarking on a transformative journey together.Data Engineer IIIIntePros is looking for a Data Engineer, to work 3 days a week on-site and 2 days a week remotely for a team located in Bellevue, WA. This is an exciting opportunity that will work with various tools and technologies automating manual tasks to reduce operational burden on business teams.Responsibilities:Work in a complex data warehouse environment. Developing and supporting analytic technologies. Design, implement, and support an analytical infrastructure providing ad-hoc access to large datasets and computing power. What are the top three MUST HAVE skill sets (technical) that are required?3+ years of Data Engineering experience. Strong SQL Skills Strong Python Skills What are the top three PREFERRED skill sets (technical)?AWS technologies like redshift, S3, AWS Glue, EMR, etc. BI report development experience. Soft Skill requirements (team fit/personality requirements)Effective communication skills Strong MS Excel skills Data analysis skills Show more Show less Seniority level Entry level Employment type Full-time Job function Information Technology Industries IT Services and IT Consulting","['Redshift', 'EMR']",2024-04-21 15:53:59.643028
Onbe,Associate Data Engineer,https://www.linkedin.com/jobs/view/associate-data-engineer-at-onbe-3900207053?position=35&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=sF%2FURYWolNPIlii%2BZXfTaA%3D%3D&trk=public_jobs_jserp-result_search-card,"Onbe, a fast-growing FinTech, bringing innovation to a rapidly growing global marketplace, stands for “on behalf.” Because that’s exactly how we work: on behalf of our clients, as their comprehensive payments partner. We transform the way payments are imagined — as an opportunity for innovation, a source of insight to customers, and a way to connect with partners around the globe!Summary: Associate Data Engineer will be part of the Data Engineering/Technology organization building ETL pipelines for reporting and other analytics needs. The position requires strong SQL skills, knowledge of data warehouse concepts, exposure to data engineering tools and technologies, willingness to learn and burning desire to grow.This role is a hybrid role that will work onsite 1-2 days per week at our Buffalo Grove, IL location. Occasional travel may be required as part of this position.Responsibilities:Collaborate with business stakeholders and Senior Data Engineers to understand data requirements and translate into technical solutions.Perform data studies and data discovery around new or existing data sources.Review existing stored procedures for troubleshooting or optimization.Develop and maintain data pipelines from SQL Server and other data sources, Transform and load to Snowflake DB.Implement ETL process using Prophecy DBT tool to create facts and dimension tables.Collaborate with cross functional teams including reporting for scalable and reliable solutions.Document source to target mappings, data pipelines, transformations as part of knowledge sharing.Participate in daily status calls, design sessions and code reviews.Qualifications:Bachelor’s degree in computer science, Information Systems, or a related field.1-3 years of experience building data transformation and processing solutions using SQL, Python or similar programming languages.1-3 years of experience with ETL tools such as Azure Data Factory, Notebook, SQL, Python, SSIS.Ability to troubleshoot and resolve complex data related issues.Excellent communication and interpersonal skills, with willingness to collaborate and the desire to learn.Knowledge of Snowflake, SQL, SSRS, SSISKnowledge of Azure data platforms and related servicesThe base salary range for this position is between $74,880.00 to $85,000.00, with eligibility for an annual bonus. The actual base salary offered depends on a variety of factors, including but not limited to the qualifications of the individual applicant for the position, years of relevant experience, specific and unique skills, level of education attained, certifications or other professional licenses held, business needs, and market demand. Our competitive benefits includes medical, dental, vision, wellness, 401(k) matching, open time off, generous parental leave, and more! Our job titles may span more than one career level. All candidates are encouraged to apply.At Onbe, a diverse group of people, ideas, and perspectives are key to achieving phenomenal things. For over 25 years, our focus has remained on building a culture of openness and ingenuity, where employees come together to innovate and build disbursement solutions that make the lives of our clients and their consumers and workforces easier and better. Our definition of success includes celebrating differences and affirming belonging. To that end, we ask employees to come to Onbe as they are and contribute their diverse perspectives, identities, and experiences.We believe that the recruiting phase is only the very beginning of diversity and inclusion. At Onbe, we’re constantly evolving the way we celebrate diversity every day and in everything we do. With several internal committees that are dedicated to mental and physical wellness, diversity, inclusion, and community outreach, we are committed to making a culture that is inclusive to all.Onbe is proud to be an equal opportunity employer. We seek out ways to create a mindful workforce that embraces diversity and celebrates a culture of inclusion. We do not discriminate against employees or job applicants on the basis of race, color, ancestry, national origin, sex (including pregnancy), gender identity, sexual orientation, marital or family status, religion, age, disability, genetic information or military service. Our equal opportunity policy applies to all decisions of employment including hiring, placement, promotion or advancement, termination, layoff, recall, transfer, compensation, training and leaves of absence Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Financial Services","['Snowflake', 'dbt']",2024-04-21 15:53:59.643028
Public Storage,Data Engineer II,https://www.linkedin.com/jobs/view/data-engineer-ii-at-public-storage-3866121228?position=36&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=IcGE84ixxHJ9Lb1Z58WBXg%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Steneral Consulting,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-steneral-consulting-3903803058?position=37&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=Cnvt2ow2E3gh4ibOF7JsYQ%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
QED National,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-qed-national-3872039504?position=38&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=uFTeUKSH054ahY28XW4RLw%3D%3D&trk=public_jobs_jserp-result_search-card,"A leading transportation authority located in New York, NY is looking for a Data Engineer for a 3 month contract.Please note that this position allows for consultant to work remotely with a very high expectation that consultant will be professional, reliable, reachable and able to work productively while remote. Will come onsite only as needed by their manager/team (at own expense)Requirements and skillsSkills and experience programming in Python and SQL – 3+ yearsSkills and experience in using data lake tools and demonstrated ability to learn new tools quicklySkills and experience using PowerBIAbility to clearly document all work (commented code, readme files, diagrams, etc.) so that work is easily transferred back to internal employeesExcellent attention to detail and QC skills to ensure errors are found and corrected before outputs are made availableGood verbal and written communication abilities for internal collaboration4-6 years' experience: Python Scripting1-2 years' experience: Adhoc SQL and Power BI ResponsibilitiesDesigning data structures and writing code to collect, combine and transform datasets to meet business needs.Developing data lake architecture to automate data extraction and transformation of raw data to more complex and calculation-based tables.Documenting work in a thorough manner consistent with team standards so that it can be easily understood by teammates and future users.Designing and carrying out testing processes and quality controls on output data for validity, accuracy and usability by the desired audience.Generating data visualization outputs Show more Show less Seniority level Associate Employment type Contract Job function Information Technology Industries IT Services and IT Consulting",['PowerBI'],2024-04-21 15:53:59.643028
Intuit,Senior Data Engineer (Mailchimp),https://www.linkedin.com/jobs/view/senior-data-engineer-mailchimp-at-intuit-3903936940?position=39&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=3V8lGznWMbP8mnfQmAtAZQ%3D%3D&trk=public_jobs_jserp-result_search-card,"OverviewIntuit is a global technology platform that helps consumers and small businesses overcome their most important financial challenges. Serving more than 100 million customers worldwide with TurboTax, Credit Karma, QuickBooks, and Mailchimp, we believe that everyone should have the opportunity to prosper. We never stop working to find new, innovative ways to make that possible.Intuit Mailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multi channel campaigns, CRM, and analytics tools.The Data Products team at Mailchimp is responsible for building capabilities on top of our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you’ll be building modeling data to feed analytical reports and data science models, optimizing and creating new ETL pipelines, or building data models in Looker or Qlik Sense. We have a lot of tools in our toolbelt, but all with the main goal of helping Mailchimp to continue to use data more effectively.What you'll bringStrong technical accomplishments in SQL and data analysis skillsExperience with data transformation orchestration tools like dbt or DataformStrong business intuition and ability to understand complex business systemsFamiliarity with additional transformation platforms like Apache Spark and BeamExperience with MPP databases such as BigQuery, Redshift, or Snowflake. We use BigQuery here at MailchimpExpertise in visualization technologies like Looker, Tableau, or Qlik SenseExperience with GCP/AWS or other cloud providers is preferredFamiliarity with Python, Go, Java or another OOP language. Most of our tools are primarily built in PythonFamiliarity with ETL orchestration tools like Apache Airflow and Google DataflowHow you will leadPartner with teams across the business and third parties to understand their needs and come up with end-to-end data solutionsModel data to create core, governed data sets for the entire company to leverageBuild scalable transformation pipelines that enable teams to easily clean up, normalize, and govern data for broad consumptionManage and model data in visualization tools, to provide a collaborative data analytics platform for the companyBuild tools and processes to help make the right data accessible to the right peopleBecome a subject matter expert on the various datasets and tools we have at MailchimpWork with Data Stewards to better document our data Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Software Development","['Snowflake', 'Redshift', 'BigQuery', 'Airflow', 'dbt', 'Tableau', 'Looker']",2024-04-21 15:53:59.643028
ICONMA,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-iconma-3891246062?position=40&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=b%2BtIiyKK9sqstLNg3aQz%2Fw%3D%3D&trk=public_jobs_jserp-result_search-card,"Key Qualifications: Excellent interpersonal and communication skillsExperience in Data Visualization (Tableau, Python required; Splunk a plus), if Javascript and its libraries (e.g., D3, ReactJS, Next.JS) a plusStrong experience and knowledge of data wrangling with proficient SQL (Trino, Postgres, Oracle required; SparkSQL, Teradata a plus)Experience using statistics to identify trends and anomalies in datasets using statistical techniques required.Experience in building robust and scalable data pipelines and ETL jobs with Python, Pandas required; Pyspark and Scala (desired)Experience in querying data through API (RESTful or GraphQL), using JSON, ProtocolBuffers, or XML desired; if with API development experience a plusExperience or working knowledge with Big Data technologies such as Hadoop, Hive, HDFS, Parquet, PySpark, and Spark desiredWorking knowledge of Kubernetes, AWS, or CI/CD development (e.g., Airflow, Jenkins) a plusExposure or interest in the UX/UI frontend development (e.g., ReactJS, Typescript) or Machine Learning (e.g., Scikit-Learn, Tensor Flow) a plusAn independent learner, insatiably curious, and a creative thinkerAbility to work both independently and within a team environmentPassion for movies, television, sports, fitness, or other video contentDescription: Develop interactive data visualizations, data pipelines/ETL jobs, and reporting to analyze and present data related to video contents, asset reviews, metadata curations, and operational supports.Closely partner with the internal teams within the AMP Video QC & Metadata Operations organization to define metrics, KPIs, and automation strategy while meeting the teams’ data and reporting needs.Automate and optimize existing data processing workloads by recognizing complex data structures and technology usage patterns and implementing solutions.Focus on scale and efficiency — build and implement innovative data solutions and establish best practices with a start-to-end workflow in mind.Education & Experience: Bachelor or Master's degree in a related field, such as Data Science, Computer Science, Statistics, Mathematics, Business Analytics, Business Administration, or meaningful industry experience preferredAs an equal opportunity employer, ICONMA pride itself on creating an employment environment that supports and encourages the abilities of all persons regardless of race, colour, gender, age, Sexual orientation, citizenship, or disabilit Show more Show less Seniority level Entry level Employment type Contract Job function Information Technology Industries Staffing and Recruiting","['Airflow', 'Tableau']",2024-04-21 15:53:59.643028
Demyst,Data Engineer (United States),https://www.linkedin.com/jobs/view/data-engineer-united-states-at-demyst-3890650480?position=41&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=6r%2Fh33nm7RtwhM2lg3kUZQ%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Accroid Inc,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-accroid-inc-3873210488?position=42&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=38ARq84SUlSwlf5t5VLREw%3D%3D&trk=public_jobs_jserp-result_search-card,"Candidate MUST HAVE financial services experience (banking or hedge funds/asset management ONLY)You will lead and deliver technical solutions for large-scale digital transformation projects. Working with the latest data technologies in the industry, you will be instrumental in helping our clients evolve for a more digital future.Your ImpactCombine your technical expertise and problem-solving passion to work closely with clients, turning complex ideas into end-to-end solutions that transform our client's businessTranslate client's requirements to system design and develop a solution that delivers business valueLead, designed, develop, and deliver large-scale data systems, data processing, and data transformation projectsAutomate data platform operations and manage the post-production system and processesConduct technical feasibility assessments and provide project estimates for the design and development of the solutionMentor, help and grow junior team membersYour Technical Skills & ExperienceDemonstrable experience in data platforms involving implementation of end to end data pipelinesHands-on experience with at least one of the leading public cloud data platforms (Azure, AWS or Google Cloud)Implementation experience with column-oriented database technologies (i.e., Big Query, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server, Oracle, MySQL)Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Azure Data Factory, Glue ETL, Lambda, Spark, Spark Streaming, etc.Ability to handle module or track level responsibilities and contributing to tasks “hands-on”Experience in data modeling, warehouse design and fact/dimension implementationsExperience working with code repositories and continuous integrationETL Automation Pipeline experience Data modeling, querying, and optimization for relational, NoSQL, timeseries, and graph databases and data warehouses and data lakesData processing programming using SQL, DBT, Python, and similar toolsData ingest, validation, and enrichment pipeline design and implementationCloud-native data platform design with a focus on streaming and event-driven architecturesTest programming using automated testing frameworks, data validation and quality frameworks, and data lineage frameworksBachelor’s degree in Computer Science, Engineering or related field. Show more Show less Seniority level Mid-Senior level Employment type Contract Job function Information Technology Industries IT Services and IT Consulting","['Redshift', 'MySQL', 'dbt']",2024-04-21 15:53:59.643028
Zortech Solutions,SQL Data Engineer (Full time),https://www.linkedin.com/jobs/view/sql-data-engineer-full-time-at-zortech-solutions-3888468133?position=43&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=ZOsg3oXJhfLbzP7NZnH6xg%3D%3D&trk=public_jobs_jserp-result_search-card,"Location: RemoteSQL Resource""Required Experience:Enterprise level support and administration focused specifically with the following: Power BI, SQL performance and tuning optimization, core sql + migration on prem and cloud, Azure data lake, Azure data factory, and SynapseStrong Business Acumen / Business Focused Technology ApplicationCandidates must be able to work independentlyAll versions of SQL Server (2012 and onwards)Solving performance problems, query tuning, and optimizationDelivering and/or authoring technical trainingSQL Server Integration Services, Reporting Services, Analysis ServicesExpert level of Microsoft enterprise software product offeringsEffective learning skillsWorks well in a team environmentAdvanced Microsoft certifications preferredTechnical SkillsMandatoryData modeling and database design.Indexing strategyPerformance Optimization and Tuning CapabilitiesPartitioning data implementationsMetadata management and repository usage.Database schema creation and management.Backup and recovery.Ensuring data integrity.Ensuring High availability and Disaster recovery.SQL code reviews with ability to dissect query plan, T-SQL codeSargability of statementsProcedural skills.Data security.Data EncryptionAuditingReplication of dataSQL Edition and version differences with discontinued and deprecated featuresUpgrade processStorage management techniques.Data Engine Capacity planning.General database management, Policy based managementExtended events and TracesBasic Licensing and support scenariosPreferredWindows Failover ClusteringSQL Server Reporting serviceSQL server Integration ServiceSQL Server Analysis ServicePowershell "" Show more Show less Seniority level Entry level Employment type Full-time Job function Other Industries IT Services and IT Consulting",['Synapse'],2024-04-21 15:53:59.643028
Cherre,"Data Engineer, Remote",https://www.linkedin.com/jobs/view/data-engineer-remote-at-cherre-3878879310?position=44&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=ACk7NXKOANCK5FVbnlrw6Q%3D%3D&trk=public_jobs_jserp-result_search-card,"Cherre is the real estate industry's leading data management platform, powering more than $3 trillion AUM globally. Our end-to-end platform helps clients connect, transform, analyze, and act on trusted data to increase efficiencies, reduce risks, gain visibility into market trends, and make strategic moves in response to changing market conditions.We are looking for an enthusiastic mid level backend/data engineer who is interested in working with a fast-growing team in building industry-leading real estate data services. You will be part of designing and implementing server side services to ingest, organize, analyze, and display real estate data and insight. You will be working in a small team and be a real partner in the design and implementation of all aspects of our product.You willDevelop and implement ETL processesDesign data warehouse solutions to support ETL processes and data analytics applicationsWrite SQL/NoSQL database queries, stored procedures, triggers, user defined functions, analytic functions, etcOwn features that you develop end to end, develop and test your code, implement new processes in production, and maintain and support them over timeDrive our data platform and help evolve our technology stack and development best practicesDevelop and unit test assigned features to meet product requirementsYou HaveStrong experience in database technologies and data warehousingStrong experience in Python or JavaA strong understanding of various data modelings techniques (3NF, Dimensional, etc.) and their intended use casesHands on experience with PostgreSQL and cloud data warehouse platforms (e.g., BigQuery, Snowflake, Redshift)Hands on experience with dbt or DataformAbility to deal with ambiguity and communicate well with both technical and non-technical teamsBenefitsCompetitive Base SalaryEquityRange of Healthcare PlansPaid Parental LeaveUnlimited VacationFlexible Work ScheduleCompensation range: $100,000-150,000 / yearIf this opportunity sounds interesting, apply or reach out to our internal talent team. We are happy to tell you more about Cherre: the technology we work with, the problems we solve, the team we are assembling, and the culture we all contribute to. We are excited you are considering working with us and look forward to hearing from you!“At the top of the mountain we are all snow leopards.” - Hunter S. ThompsonCherre is an equal opportunity employer. We pride ourselves on hiring the best people for the job no matter their race, sex, orientation, nationality, religion, disability, or age. Show more Show less Seniority level Not Applicable Employment type Full-time Job function Information Technology Industries Real Estate","['PostgreSQL', 'Snowflake', 'Redshift', 'BigQuery', 'dbt']",2024-04-21 15:53:59.643028
LTIMindtree,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-ltimindtree-3903809494?position=45&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=GCXIsVBbrUdZG0rBM4k3Lw%3D%3D&trk=public_jobs_jserp-result_search-card,"About Us:LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.Job Title: Data engineerWork LocationBellevue, WAJob Description:# Experience in SQL Programming language.# Knowledge of end-to-end process on creation, maintenance of Data pipelines# Working experience on Azure DevOps is a must# Working Knowledge on Azure Data factory, Azure Data Lake and Azure Data lake storage.# Proficiency in Azure synapse# Implement end-end data pipelines using cosmos Azure Data factory.# Experience on Version control ADO GIT, CI CD.# Should have good analytical thinking and Problem solving.# Ability to showcase data analysis effectively using KPI’s, metrics, charts# Experience with Data quality implementations assessing data correctness, completeness, uniqueness etc# Experience working on PII, GDPR, handling sensitive data (encryption/decryption)# Able to work as Individual contributor and with offshore teamResponsibilities/Expectations From The Role# Good communication and co-ordination skills.# Requirement Analysis and questioning skills# Create Maintain and Enhance Data Pipeline.# Daily status reporting, interacting with Leads.# Data Platform, Product telemetry, Analytical thinking.# Data Validation of the new streams.# Data quality check of the new streams#debugging of issues and making good quality code fixes.# Monitoring of data pipeline created in Azure Data factory.# updating the Tech spec and wiki page for each implementation of pipeline.# Updating ADO on daily basis.Good to have# Hands-on in Visualization like PowerBI.# Marketing Campaign experiencesTechnical Skillset (Required): Azure Data Factory, EDL, SQL Server 2016, Azure Synapse, , Azure Cosmos DB, Azure Databricks, PySpark, Python, Excel, Azure Databricks, EDL.Technical Skillset (Good to have): Power BI & Azure synapseBenefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):Benefits and Perks: Comprehensive Medical Plan Covering Medical, Dental, Vision Short Term and Long-Term Disability Coverage 401(k) Plan with Company match Life Insurance Vacation Time, Sick Leave, Paid Holidays Paid Paternity and Maternity LeaveThe range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.Safe return to office:In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes. Show more Show less Seniority level Entry level Employment type Full-time Job function Information Technology Industries IT Services and IT Consulting","['Databricks', 'PowerBI', 'Synapse']",2024-04-21 15:53:59.643028
Adame Services LLC,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-adame-services-llc-3891475109?position=46&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=HcgCBuwjDaKgHbEbf3v6xw%3D%3D&trk=public_jobs_jserp-result_search-card,"candidates are muat be local to ColumbusData EngineerFully remote.Our direct client is looking for a Data Engineer to help support the IT organization’s data strategy in support of their business customers, as well as internal initiatives.If you are a “Data Person”, who knows how to build data pipelines, aggregate data from multiple sources into a common data store and have experience with industry-leading cloud data platforms and scripting, we’d love to talk with you.The Data Engineer will work closely with Client’s IT Data Architect, Integration Engineers, Business Operations customers, and Services Partners to help advance their data and analytics ambitions.This role will serve as a hands-on contributor and be a thought leader in the areas of data engineering, cloud data strategy, Business Intelligence, Data Modeling and ETL/ELT.Responsibilitieswith the IT Data Engineering Team and assist in developing the next generation data and analytics infrastructure.have strong SQL modeling skills (dbt is a plus)high quality SQL code to retrieve and analyze data from database tables (primarily Databricks)high quality SQL models for ad-hoc requests, as well as ongoing reporting / dashboarding.directly with business stakeholders to translate between data and business needs.improve SQL models through automating or simplifying self-service support for datasetsRequirementsBachelor's degree in Computer Science, Information Systems, Engineering, Data Science, or other similarly technical related field Mathematics, or specialized training/certification. Or equivalent work experience.1- 3-year minimum professional experience in cloud data engineering and science and associated technologies utilizing cloud data platforms such as Databricks, AWS RedShift, and Snowflake.1 - 3-year minimum experience with advanced SQL data modeling and query optimizationProficiency in using visualization tools such as Tableau, Domo, or Power BI.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong verbal, written & presentation skills with the ability to effectively communicate complex technical information to personnel at all levels of the organization.Nice To HaveSpecific experience with Data Warehouse/Data Lake configuration and development using Databricks platform.Experience with Tableau / Sigma Computingoperating in an Agile development environment.with usage of Agile tools (JIRA / Confluence)of CI/CD deployment models and release strategy as well as SCM tools (Git preferred) and code management best practices.in AWS environment.with cloud ELT platforms such as AWS Glue, Talend Stitch, or FiveTran Show more Show less Seniority level Mid-Senior level Employment type Contract Job function Information Technology Industries IT Services and IT Consulting","['Snowflake', 'Databricks', 'Redshift', 'dbt', 'Fivetran', 'Tableau', 'Talend']",2024-04-21 15:53:59.643028
Bosch USA,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-bosch-usa-3892754607?position=47&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=kM8SyAJsQpe5dHOdmfe3zw%3D%3D&trk=public_jobs_jserp-result_search-card,"Company DescriptionDo you want beneficial technologies being shaped by your ideas? Whether in the areas of mobility solutions, consumer goods, industrial technology or energy and building technology _ with us, you will have the chance to improve quality of life all across the globe. Welcome to Bosch. The Lincolnton, NC Conversion, Customizing and Packaging Center is located approximately 30 miles from Charlotte. The facility employs just under 500 full time associates in a 5-shift operation, which encompasses 235,000 sq ft. along with 12,000 skus. In addition to the conversion, customizing and packaging activities, abrasives are now a big part of our power tool accessory business. ISO certified, our associates engage in BPS, CIP and 5S . By choice, we are committed to a diverse workforce - EOE/Protected Veteran/DisabledJob DescriptionDatabase and server administration including, but not limited to developing data pipelines for data extraction, transformation, and integration, as well as their respective storage, in a robust and sustainable architecture. Provides technical assistance and troubleshooting support of server and database – related topics.Primary Duties And AccountabilitiesRole will be responsible for core data ETL across the organization.Use SQL to extract custom data sets from enterprise-wide data lakes, deploy them as data cubes which can be accessed via spreadsheets, Power BI or other analytical platforms.Effectively engage with multiple stakeholders across manufacturing, technical functions, controlling and IT teams.Clean and transform extracted data by handling missing values and inconsistencies,Perform data aggregation, analysis, and transformation using statistical methods and data manipulation tools.Ensure data integrity and security.Collaborate with database administrators and engineers to optimize data storage and access.Actively seek standardization and automation, apply, and implement best practices across finance functions to streamline the entire analytics process.QualificationsBasic Qualifications:Education: Bachelor's or Master's degree in Computer Science, Electric Engineering, other Engineering discipline or foreign equivalent3+ years of experience with building data pipelines within a Cloud or Cloud-hybrid setup; in-depth understanding of relational database systems (e.g. Oracle, MS SQLServer).3+ years of experience with distributed computing frameworks (e.g. k8s, Spark)3+ years of experience in object-oriented software development, (e.g. Python, Java, or Go).2+ years of experience with Linux.PreferredEducation: successfully completed Master's degree in Computer Science or other engineering disciplineExperience with recent non-relational storage technologies (NoSQL and distributed)Experience with workflow automation tools (e.g. Jenkins, Ansible)Experience with Nexeed MES PlatformExperience with various messaging systems (e.g. Kafka)Experience in designing data models and choice of respective data formatsExperience with in-vehicle data collection skills, structured and analytical connectivityAdditional InformationAll your information will be kept confidential according to EEO guidelines. Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Quality Assurance Industries Mechanical Or Industrial Engineering",['Kafka'],2024-04-21 15:53:59.643028
Palmetto,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-palmetto-3890889737?position=48&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=xUqQTJBuLFirsluy3IG08g%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Strava,Data Engineer II,https://www.linkedin.com/jobs/view/data-engineer-ii-at-strava-3903879731?position=49&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=Jxcm%2B003n9xzmJ88gFJS%2FA%3D%3D&trk=public_jobs_jserp-result_search-card,"About This RoleStrava is the leading digital community for active people with more than 120 million athletes, in more than 190 countries. The platform offers a holistic view of your active lifestyle, no matter where you live, which sport you love and/or what device you use. Everyone belongs on Strava when they are pursuing an active life.We are seeking data engineers to join our Data Platform team. Our vision is that key decisions and product at Strava can be greatly enriched with data to benefit athletes and the business. Our mission is to build and support a platform that enables access to data through self-service and extensible tools.This means we listen to all corners of the company for opportunities where data can make a difference. We distill this down and use modern technologies to develop both generalized and special-purpose data solutions.For more information on compensation and benefits, please click here.This is a hybrid role based in our San Francisco office.You’re excited about this opportunity because you will:Collaborate with people across teams and functions that hold deep curiosity for data.Work with hefty data systems at the global scale of Strava.Deliver value more through software, leaning into tooling and automation rather than repetitive toil.Grow your expertise in the steadily evolving technologies and ecosystem of data.You will be successful here by:Holding empathy for the users of our platform to truly understand the challenges we tackle for them.Fostering an inclusive and motivating team culture to help everyone achieve their best.Caring about high quality and reliable code, but also the end user experience.Solving problems efficiently, creatively, and completely despite constraints in time or resources.Understanding how critical it is we maintain a high bar of data security and privacy.We’re excited about you because you:Have the ability to adapt and apply evolving data technologies to business needs (which means the list of bullets below will change over time!).Have developed software using programming languages like Python, Scala, Java, Go, Ruby, etc.Have sufficient familiarity to understand SQL queries in the context of data pipelines (i.e. dbt).Have experience with distributed data tools (i.e. Spark, Flink, Kafka) on large datasets.Have worked with cloud-data warehouses (i.e. Snowflake, BigQuery, Redshift) or other warehousing solutions.Have an understanding of underlying infrastructure needed to serve production services (i.e. Kubernetes, AWS, GCP, Azure).About StravaStrava is Swedish for “strive,” which epitomizes who we are and what we do. We’re a passionate and committed team, unified by our mission to connect athletes to what motivates them and help them find their personal best. And with billions of activity uploads from all over the world, we have a humbling and audacious vision: to be the record of the world’s athletic activities and the technology that makes every effort count.Strava builds software that makes the best part of our athletes’ days even better. And just as we’re deeply committed to unlocking their potential, we’re dedicated to providing a world-class, inclusive workplace where our employees can grow and thrive, too. We’re backed by Sequoia Capital, Madrone Partners and Jackson Square Ventures, and we’re expanding in order to exceed the needs of our growing community of global athletes. Our culture reflects our community – we are continuously striving to hire and engage diverse teammates from all backgrounds, experiences and perspectives because we know we are a stronger team together.Despite challenges in the world around us, we are continuing to grow camaraderie and positivity within our culture and we are unified in our commitment to becoming an antiracist company. We are differentiated by our truly people-first approach, our compassionate leadership, and our belief that we can bring joy and inspiration to athletes’ lives — now more than ever. All to say, it’s a great time to join Strava!Strava is an equal opportunity employer. In keeping with the values of Strava, we make all employment decisions including hiring, evaluation, termination, promotional and training opportunities, without regard to race, religion, color, sex, age, national origin, ancestry, sexual orientation, physical handicap, mental disability, medical condition, disability, gender or identity or expression, pregnancy or pregnancy-related condition, marital status, height and/or weight.We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.California Consumer Protection Act Applicant Notice Show more Show less Seniority level Entry level Employment type Full-time Job function Information Technology Industries Software Development","['Snowflake', 'Redshift', 'BigQuery', 'Kafka', 'dbt', 'NiFi']",2024-04-21 15:53:59.643028
Accenture,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-accenture-3899960278?position=50&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=7kWdZ9mg0y0qg%2Blm4tSqjA%3D%3D&trk=public_jobs_jserp-result_search-card,"Accenture Flex offers you the flexibility of local fixed-duration project-based work powered by Accenture, a leading global professional services company. Accenture is consistently recognized on FORTUNE's 100 Best Companies to Work For and Diversity Inc's Top 50 Companies For Diversity lists.As an Accenture Flex employee, you will apply your skills and experience to help drive business transformation for leading organizations and communities. In addition to delivering innovative solutions for Accenture's clients, you will work with a highly skilled, diverse network of people across Accenture businesses who are using the latest emerging technologies to address today's biggest business challenges.You will receive competitive rewards and access to benefits programs and world-class learning resources. Accenture Flex employees work in their local metro area onsite at the project, significantly reducing and/or eliminating the demands to travel.Key ResponsibilitiesUtilize strong SQL Python expertise to engineer sound data pipelines and conduct routine and ad hoc analysis to assess the performance of legacy products and the saliency of new features.Build reporting dashboards and visualizations to design, create and track campaign program KPIs Perform analyses on large data sets to understand drivers of marketing engagement and provide recommendations on campaign and product optimization. Show more Show less Seniority level Entry level Employment type Full-time Job function Information Technology Industries Business Consulting and Services",['NiFi'],2024-04-21 15:53:59.643028
"Triunity Software, Inc.",Jr. Data Engineer,https://www.linkedin.com/jobs/view/jr-data-engineer-at-triunity-software-inc-3866683332?position=51&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=15j7gL4HWAhek6sW72oksg%3D%3D&trk=public_jobs_jserp-result_search-card,"Direct message the job poster from Triunity Software, Inc. Sai kiran Doli Sai kiran Doli SAFe Certified | Talent Discovery analyst | Focusing on acquiring top talent for the Company. Location: Harrison, NJ Type: W23 to 4 years of Relevant experience in Data Engineering and delivery.Working Experience in SQL Spark Python and snowflakeGood aptitude, strong problem-solving abilities, and analytical skills, ability to take ownership as appropriate.Should be able to do coding, debugging, performance tuning, and deploying the apps to the Production environment.Experience working in Agile MethodologyAbility to learn and help the team learn new technologies quickly. Show more Show less Seniority level Associate Employment type Full-time Job function Information Technology Industries Technology, Information and Media",['Snowflake'],2024-04-21 15:53:59.643028
Raintree Systems,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-raintree-systems-3880307475?position=52&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=mLFgiyHy%2Fw15z%2BV1d4uAtQ%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Worth AI,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-worth-ai-3891688319?position=53&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=nPRECMTgW1yrtO5Z9wYYmw%3D%3D&trk=public_jobs_jserp-result_search-card,"Worth AI, a dynamic player in the computer software industry, is seeking a skilled and motivated individual to join their team as a Data Engineer. Worth AI is committed to transforming decision-making using the power of AI while promoting equity, diversity, and inclusion. With core values centered around diversity of thought, teamwork, and adaptability, Worth AI is dedicated to making a positive impact in the business world.As a Data Engineer at Worth AI, you will play a crucial role in designing and implementing data pipelines, data integration solutions, and data infrastructure to support the company's AI initiatives. You will collaborate closely with data scientists, software engineers, and other stakeholders to ensure effective data management and accessibility. Your expertise in data processing, data modeling, and database optimization will be essential in delivering scalable and reliable data solutions.ResponsibilitiesDesign, build, and maintain large-scale data processing systems and architectures that support AI initiativesDevelop and implement data pipelines and ETL processes to ingest, transform, and load data from various sourcesDesign and optimize databases and data storage solutions for high performance and scalabilityCollaborate with cross-functional teams to understand data requirements and ensure data quality and integrityImplement data governance and data security measures to protect sensitive dataMonitor and troubleshoot data infrastructure and pipeline issues in a timely mannerStay up-to-date with the latest trends and technologies in data engineering and recommend improvements to enhance the company's data capabilitiesRequirementsProven experience as a Data Engineer or similar role, preferably in a software or technology-driven company with experience processing several to several hundreds of Gigabytes of data or moreIn-depth knowledge of data modeling, data warehousing, and database design principlesStrong programming skills in Python, SQL, and other relevant languagesExperience with relational and NoSQL databases, such as PostgreSQL, MySQL, MongoDBProficiency in data integration and ETL tools, such as Apache Kafka, Apache Airflow, or InformaticaFamiliarity with big data processing frameworks, such as Hadoop, Spark, or FlinkKnowledge of cloud platforms, such as AWS, Azure, or GCP, and experience with data storage and processing services in the cloudUnderstanding of data governance, data privacy, and data security best practicesStrong problem-solving and troubleshooting skills, with a focus on data quality and system performanceExcellent communication and collaboration skills to work effectively with cross-functional teamsPrior collaborative work with data scientists or machine learning professionals with respect to sourcing, processing and scaling both input and output dataComfortable going through documentation of third-party API’s and identifying best procedures for integrating data from API’s into broader ETL processes BenefitsHealth Care Plan (Medical, Dental & Vision)Retirement Plan (401k, IRA)Life InsuranceUnlimited Paid Time Off9 paid HolidaysFamily LeaveWork From HomeFree Food & Snacks (for those who are in Orlando, FL)Wellness Resources Show more Show less Seniority level Entry level Employment type Full-time Job function Engineering Industries Technology, Information and Internet","['PostgreSQL', 'MongoDB', 'MySQL', 'Kafka', 'Airflow', 'Informatica']",2024-04-21 15:53:59.643028
Janus Health,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-janus-health-3894594055?position=54&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=7XwepZd1O6AHhO1nSB9gTA%3D%3D&trk=public_jobs_jserp-result_search-card,"At Janus, we believe in a world where healthcare functions efficiently. Join us on our mission to improve the lives of administrative workers and fundamentally change the way work is done. Our team is building a world-class process improvement platform to help healthcare providers generate more cash with fewer resources.SummaryWe are seeking a skilled and detail-oriented Data Engineer to join our team! Our Data Engineers play a crucial role in building highly reliable and scalable data pipelines to support our automation systems. This role is responsible for transforming raw data into useful data systems, developing algorithms, conducting statistical analysis, and building robust data pipelines. This is an exciting opportunity to work closely with our Product and Engineering teams, as well as collaborate with clients to drive data-driven insights and solutions. The ideal candidate will have a strong analytical mindset, excellent organizational skills, and a deep understanding of data engineering principles.ResponsibilitiesBuild and maintain efficient data systems and pipelines that support our automation systems. Collaborate with the Product and Customer Success teams, as well as clients, to understand business needs and objectives. Analyze data trends and patterns to identify valuable insights and opportunities for improvement. Conduct complex data analysis, generate reports, and present findings to stakeholders. Prepare data for prescriptive and predictive modeling, working closely with data scientists and analysts. Design and develop algorithms and prototypes to enhance data processing and analysis capabilities. Integrate and combine raw data from various sources to create comprehensive and reliable datasets. Identify and implement strategies to improve data quality, integrity, and reliability. Proactively explore opportunities for data acquisition to enrich our datasets. Develop and maintain analytical tools and programs to support data analysis and reporting. Collaborate effectively with data scientists and architects on multiple projects. Qualifications1-3 years experience as a Data Engineer or similar role, preferably in a fast-paced environment. Bachelor's degree in computer science or related field and/or equivalent work experience is preferred.Strong proficiency in data engineering technologies and tools (e.g., SQL, Python, ETL frameworks). Experience with cloud platforms, specifically AWS (e.g., Quicksight, Databricks), and big data technologies (e.g., Hadoop, Spark). Solid understanding of data analysis, data modeling, and database design principles. Experience in building and optimizing data pipelines and workflows. Experience in statistical analysis and data visualization techniques. Strong analytical skills and the ability to interpret complex data sets. Detail-oriented with excellent problem-solving and organizational abilities. Strong communication skills and the ability to collaborate effectively with cross-functional teams. Ability to adapt and learn quickly in a dynamic and evolving data environment. Has fun, celebrates success, and contributes to a positive culture! We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies. Care for the Whole PersonAt Janus, our commitment is to provide each employee with what they need to be successful. Our benefits package has been designed in a thoughtful way that allows our employees to be happy, healthy and whole. Here are a few things we offer:We contribute 100% of base plan (HDHP) medical premiums for employees and 50% of premiums for family members. There are other options available as well.We contribute 75% of premiums for dental and vision insurance for employee-only plans.We have an employee assistance program that allows you the chance to work through any issues that may arise with the appropriate professional.We have a 401k plan with minimal portfolio fees, traditional and roth options, as well as rollovers and loan capabilities.We offer stock options to share in the value we create and in the ownership of Janus, so let's make it something that we are proud of.We offer unlimited PTO because we want our employees to take the time they need to rejuvenate and relax. At minimum, encourage all employees to take at least 15 fully unplugged days off each year.We encourage sacred moments that are free from distractions and allow you to create a connection with someone or something that is meaningful to you.We provide a monthly allowance to cover the cost related to working in a remote environment like upgraded internet or to offset your cell phone bill.We offer parental leave because bonding with your newest addition is so important! We have caregiving leave for our employees that are the primary caregiver for a loved one and needs time to care for that person.We want you to look for personal enrichment opportunities and will give you up to $500 per year to invest in yourself. We want you to be the best version of you and take time to do things you enjoy!We encourage on-going training, additional certifications and professional development related to your role and will review all requests for additional growth (including travel).We have committees focused on organizational initiatives to increase employee happiness and the recruitment and retention of a broad, inclusive workforce that represents a diverse range of interests, abilities, talents, and cultures. You are welcome to join!We have a benefits summary for you to review and will send more comprehensive information with your offer letter. If you want to review it sooner, just let us know!Equal Opportunity StatementJanus is an equal opportunity employer. We hire great people from a wide variety of backgrounds and appreciate our differences. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.We consider qualified applicants regardless of criminal histories, consistent with legal requirements.Further, consistent with applicable federal and state law, Janus provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Janus' policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require reasonable accommodation, please contact the People team.E-VerifyThis employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form. Show more Show less Seniority level Entry level Employment type Full-time Job function Information Technology Industries Technology, Information and Internet",['Databricks'],2024-04-21 15:53:59.643028
Capital Fund Management (CFM),Data Engineer - New-York,https://www.linkedin.com/jobs/view/data-engineer-new-york-at-capital-fund-management-cfm-3889322683?position=55&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=serGU79FZb%2FsN9W7PScXzg%3D%3D&trk=public_jobs_jserp-result_search-card,"Direct message the job poster from Capital Fund Management (CFM) Anne V. Anne V. Head of Talent Acquisition and Talent Management ABOUT CFMFounded in 1991, we are a global quantitative and systematic asset management firm applying a scientific approach to finance to develop alternative investment strategies that create value for our clients.We value innovation, dedication, collaboration and the ability to make an impact and together we create an environment for talented and passionate experts in research, technology and business to explore new ideas and challenge assumptions.ABOUT THE ROLEAs pioneers in scientific, quantitative trading, we explore more and more datasets in order to shape and consolidate our trading decisions. The Data Technology team works closely with Research teams to integrate new datasets and make sure data integration pipelines run smoothly and efficiently.Data is the fuel that powers our investment strategies: intraday price dynamics are used to better our investment decisions; buy/sell intents of market participants help to trade at the best available price…You will join our New York office.Base salary 105-170 K-USD+ variable compensation+ competitive benefit packageYour main responsibility will be to make sure that they have all the data and tools they need to improve our trading strategies, including:Integration of ‘trial’ datasetsDevelopment & monitoring of data pipelinesTechnical support to quant researchersContribution to our platform toolingWe choose between Python and C++ for our developments based on the technical requirements with occasional Linux bash scripting and SQL. The job will involve a diverse set of tasks, including collaboration with quant research, so there will be plenty opportunities to grow.SKILLSET REQUIREMENTS/QUALIFICATIONSYou have a Master Degree from an engineering school or university equivalentYou have at least 3 years experience in data analysis or similar roleYou have a working visa allowing you to work in the USAYou have experience in PythonYou have an ability to work on Linux environmentYou are a team player and have good communication skillsYou are interested in technologyYou are fluent in written & spoken EnglishYour “plusses : ”C++ not required but is a plusKnowledge of French is a plusEQUAL OPPORTUNITIES STATEMENTWe are continuously striving to be an equal opportunity employer and we prohibit any discrimination based on sex, disability, origin, sexual orientation, gender identity, age, race, or religion. We believe that our diversity, breadth of experience, and multiple points of view are among the leading factors in our success.CFM is a signatory of the Women Empowerment PrinciplesFOLLOW USFollow us on Twitter and LinkedIn or visit our website to find out more about CFM. Show more Show less Seniority level Associate Employment type Full-time Job function Engineering, Information Technology, and Finance Industries Investment Management, Financial Services, and Investment Banking",[],2024-04-21 15:53:59.643028
Rearc,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-rearc-3901573647?position=56&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=w4uQ7nRmndO6eCfZd%2Bh9jg%3D%3D&trk=public_jobs_jserp-result_search-card,"About RearcRearc was founded with people and engineering at its core. We partner with clients to drive their business outcomes with best-in-class solutions tailored to their needs.We look for team members who value ownership and freedom with outstanding leadership abilities and expertise in their field.About The RoleAs a Data Engineer at Rearc, you will play a pivotal role in architecting, building, and maintaining data infrastructure and pipelines. Collaborating with both internal teams and clients, you will ensure efficient data processing, storage, and accessibility while adhering to industry best practices and security standards.We are building a team of knowledgeable and competent data engineers that build best-in-class solutions to help our customers realize the full potential of data in their business. We use cloud-native architectures and modern data engineering practices to help data engineers, data scientists, and business analysts realize their full potential.What You'll DoEvaluate clients' existing data infrastructure and provide recommendations for improvements in areas such as scalability, reliability, and performance.Design, implement, and maintain data pipelines to ingest, transform, and load data into various data stores and warehouses.Develop and optimize data models to support analytics, reporting, and machine learning applications.Design, implement, and maintain data pipelines to ingest, transform, and load data into various data stores and warehouses, leveraging Databricks and Apache Spark technologies.Implement and enforce data governance policies and procedures to ensure data integrity, privacy, and compliance with regulatory requirements.Stay abreast of emerging technologies and trends in data engineering and share knowledge with colleagues and clients.We're Looking ForStrong proficiency in data processing concepts and technologies, including data warehousing, ETL/ELT, data modeling, and SQL.Experience with programming languages commonly used in data engineering, such as Python, Scala, or Java.Familiarity with cloud platforms like AWS, GCP, or Azure, and their respective data services (e.g., AWS Glue, GCP Dataflow, Azure Data Factory).Hands-on experience with big data technologies like Hadoop, Spark, or Kafka.Ability to work collaboratively in a team environment, with excellent communication and interpersonal skills.Comfortable with remote communication and collaboration tools, including video conferencing.Proficiency in English, both written and verbal.Bonus Points ForExperience with distributed computing frameworks like Apache Spark or Apache Flink.Knowledge of containerization technologies (e.g., Docker, Kubernetes) and container orchestration platforms.Background in software development or data science.Certifications in relevant technologies or cloud platforms.Your first few weeks at Rearc will be spent in an immersive learning environment where our team will help you get up to speed. Within the first few months, you’ll have the opportunity to experiment with a lot of different tools as you find your place on the team.Benefits And PerksHealth BenefitsGenerous time awayMaternity and Paternity leaveEducational resources and reimbursements401(k) plan with a company contributionRearc is committed to a diverse and inclusive workplace. Rearc is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.The Pay Range For This Role Is100,000 - 140,000 USD per year(NomadWorks: Virtual Office) Show more Show less Seniority level Entry level Employment type Full-time Job function Information Technology Industries IT Services and IT Consulting","['Databricks', 'Kafka']",2024-04-21 15:53:59.643028
PrismHR,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-prismhr-3876839265?position=57&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=GbZ0GSqFKAQlVSoYsyBhAA%3D%3D&trk=public_jobs_jserp-result_search-card,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feedsDefining streaming event data feeds required for real-time analytics and reportingLeveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performanceAs a Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product.Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!Responsibilities:Build our next generation data warehouseBuild our event stream platformTranslate user requirements for reporting and analysis into actionable deliverablesEnhance automation, operation, and expansion of real-time and batch data environmentManage numerous projects in an ever-changing work environmentExtract, transform, and load complex data into the data warehouse using cutting-edge technologiesBuild processes for topnotch security, performance, reliability, and accuracyProvide mentorship and collaborate with fellow team membersQualifications:Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required3+ years of experience building data pipelines3+ years of experience building data frameworks for unit testing, data lineage tracking, and automationFluency in Scala is requiredWorking knowledge of Apache SparkFamiliarity with streaming technologies (e.g., Kafka, Kinesis, Flink)Nice-to-Haves:Experience with Machine LearningFamiliarity with Looker a plusKnowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between $110,000 - $130,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description).PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners.Diversity, Equity and Inclusion Program/Affirmative Action Plan:We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers.Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.Powered by JazzHRHEB5qlUshT Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Internet Publishing","['Kafka', 'Kinesis', 'Looker']",2024-04-21 15:53:59.643028
"HireIO, Inc.",Data Engineer for Data Platform,https://www.linkedin.com/jobs/view/data-engineer-for-data-platform-at-hireio-inc-3887627940?position=58&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=cDwQL2Gu0g6CiE1PDwhUXA%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Inceed,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-inceed-3867279168?position=59&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=9eHV%2F0cBiZ04mwLEhYeF3g%3D%3D&trk=public_jobs_jserp-result_search-card,"Compensation: $130-150k/yearLocation: RemoteData Engineer:Inceed has partnered with a great company to help find a skilled Data Engineer to join their team!The Data Platform Engineer be instrumental in designing, implementing, and optimizing our data infrastructure to support critical business operations and analytics.Responsibilities:Design, develop, and maintain high-performance data pipelines to ingest, process, and analyze large volumes of data.Collaborate with data scientists, analysts, and software engineers to understand data requirements and translate them into technical solutions.Architect scalable and fault-tolerant systems on cloud data platforms (e.g., AWS, Azure, Google Cloud) to support OLAP workloads.Implement best practices for data modeling, schema design, and optimization to ensure efficient data storage and retrieval.Build reusable infrastructure components and frameworks to accelerate development and promote code reuse.Optimize data infrastructure for performance, scalability, and cost-effectiveness.Qualificatons:Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field.Experience in building data pipelines using Python.Strong understanding of system design principles and experience in designing scalable, distributed systems.Hands-on experience with OLAP technologies (e.g., Apache Kylin, Druid) and cloud data platforms (e.g., AWS Redshift, Google BigQuery).Proficiency in SQL and experience with database technologies (e.g., PostgreSQL, MySQL).Familiarity with containerization technologies (e.g., Docker, Kubernetes) and orchestration tools (e.g., Apache Airflow).Excellent problem-solving skills and ability to troubleshoot complex issues in a distributed environment.Strong communication and collaboration skills with the ability to work effectively in a team environment.If you are interested in learning more about the Data Engineer opportunity, please submit your resume for consideration. Our client is unable to provide sponsorship at this time.We are Inceed, a staffing and direct placement firm who believes in the possibility of something better. Our mission is simple: We’re here to help every person, whether client, candidate, or employee, find and secure what’s better for them. Inceed is an equal opportunity employer. Inceed prohibits discrimination and harassment of any type and affords equal employment opportunities to employees and applicants without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law. Show more Show less Seniority level Entry level Employment type Full-time Job function Information Technology Industries Staffing and Recruiting","['PostgreSQL', 'Redshift', 'BigQuery', 'MySQL', 'Airflow']",2024-04-21 15:53:59.643028
Netflix,Data Engineer (L5) - Privacy,https://www.linkedin.com/jobs/view/data-engineer-l5-privacy-at-netflix-3877636194?position=60&pageNum=0&refId=gSefy%2BzNvtyGZF4PXvW%2FDQ%3D%3D&trackingId=Fr84r6SN0rqnGbS0cMKZ0Q%3D%3D&trk=public_jobs_jserp-result_search-card,"Netflix is enjoyed by more than 230 million members globally, entertaining new audiences every day. We are in the midst of major transformative developments for the Netflix product with the launch of an Advertising-supported plan, Games, and Live content. These new products, alongside our streaming service, have resulted in significant increases in the complexity and breadth of our internal data ecosystem. We manage one of the largest paid subscription businesses and are committed to handling our members’ data with a high degree of care towards appropriate use as well as compliance with local privacy laws and regulations in the 190+ countries where we operate.The Privacy and Legal Data Engineering pod builds scalable data management and extraction frameworks which are at the core of our ability to hold ourselves to the highest data privacy and hygiene standards in the industry.We are looking for a Data Engineer to help augment our ability to build these robust, scalable privacy-centric data frameworks. As part of this team, you will work on diverse data technologies such as Spark, Presto, Flink, Kafka, and others to build insightful, scalable, and robust data pipelines; write ETL jobs to collect and aggregate data; and build high-quality data frameworks that enable appropriate handling of customer personal data.The ideal candidate will bring a strong track record of having built data systems and frameworks to strengthen the privacy posture at large consumer businesses. They will have a deep background in distributed data processing and share our passion for continuously improving the ways we handle data to make Netflix's data privacy posture better.Who You ArePassionate about consumer-centric data privacy and risk mitigation for the businessHighly proficient in at least one of Java, Python, or Scala with at least 10 years of software/data engineering experienceProficient in advanced SQL and effective in a complex data environmentHighly experienced in engineering data pipelines using big data technologies (Hive, Presto, Spark, Flink) on medium to large scale data setsComfortable working cross-functionally with multiple types of stakeholder groupsAble to successfully lead large, complex systems design and implementation challenges independentlyWhat You Will DoDesign and implement elegant frameworks to scalably meet various internal and consumer-facing data privacy and legal needsEngineer efficient, adaptable and scalable data pipelines to process structured and unstructured data Develop a deep understanding of the data ecosystem at Netflix from a privacy lensPartner with the privacy engineering teams to understand product goals and provide data that enables us to respond to customer and regulatory data requests Mentor and inspire teammates while elevating the impact of the teamEnable Netflix to effectively manage legal and regulatory risk and compliance while maintaining a high standard of consumer data privacy expectationsA Few More Things To KnowOur culture is unique and we live by our values, so it's worth learning more about Netflix at jobs.netflix.com/culture. We regularly share examples of our work on our tech blog. You will need to be comfortable working in the most agile of environments. Requirements will be vague. Iterations will be rapid. You will need to be nimble and take smart risks.Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top-of-market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more details about our Benefits here.Netflix is a unique culture and environment. Learn more here.We are an equal opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service. Show more Show less Seniority level Not Applicable Employment type Full-time Job function Information Technology Industries Entertainment Providers, Technology, Information and Internet, and Movies, Videos, and Sound","['Kafka', 'NiFi', 'presto']",2024-04-21 15:53:59.643028
Parallel Consulting,Junior Data Engineer,https://www.linkedin.com/jobs/view/junior-data-engineer-at-parallel-consulting-3868478683?position=1&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=TfSYDM9mIbKi6lnA3TcoBQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Junior Data EngineerThe FirmWe are working with the Director of Data Science of a boutique management consulting firm that is continuing to grow its Analytics and Engineering team. This is a great opportunity to join a firm that uses economics, statistics, and data science to solve problems for our federal and private-sector clients.As Junior Data Engineer, you would be joining a cross-functional team of data scientists, other engineers, and business analysts, and be responsible for supporting the build of data integration solutions in a modern cloud environment that help their clients analyze data more efficiently.RequirementsBachelor's Degree and at least 2 years of experience in programming in Python and SQLExperience with database designs, implementing and managing data pipelines within a cloud environment (AWS/Azure preferred)Skills in data modeling, statistical modeling, and/or data visualizations would be valuableWe do require US Citizens for this role that have an ability to obtain a public trust clearance.Location: DC/NoVA – Flexible work environment (1-2 days per week in office)Comp: $90-100k Base + Bonus Show more Show less Seniority level Associate Employment type Full-time Job function Information Technology Industries Business Consulting and Services",[],2024-04-21 15:53:59.643028
"eTek IT Services, Inc.",Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-etek-it-services-inc-3900983042?position=2&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=ZWjvI0AHlBQoPZO0Mww%2BPA%3D%3D&trk=public_jobs_jserp-result_search-card,"Job Description Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Extensive Experience on BigQuery, DataProc and DataFlow platforms on Google Cloud platform. Having experience on Azure Databricks is an added advantage (not mandatory). Experience on Cluster capacity configurations and cloud optimization to meet application demand. Programming experience on Python, Shell scripting, PySpark and other data programming language. Programming experience on Apache Beam Java SDK for building effective heavy data piplines and deploying them in GCP DataFlow. CICD process to deploy these pipelines in GCP. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with Data Visualization Dashboard, Metrics and etc. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores. Familiar with Deployment tool like Docker and building CI/CD pipelines. Experience supporting and working with cross-functional teams in a dynamic environment. 8+ years' experience in software development, Data engineering, and Bachelor's degree in computer science, Statistics, Informatics, Information Systems or another quantitative field. Postgraduate/master's degree is preferred. Experience in Machine Learning and Data Modeling is a plus.The ideal resource would be local to the Dallas, TX area so they could be in office 1-2 days per week. They would have extensive experience on BigQuery, DataProc and DataFlow platforms on Google Cloud platform. Having experience on Azure Databricks is an added advantage (not mandatory). Programming experience on Python, Shell scripting, PySpark and other data programming language. Programming experience on Apache Beam Java SDK for building effective heavy data pipelines and deploying them in GCP DataFlow. CICD process to deploy these pipelines in GCP.Skills: gcp,cloud,azure Show more Show less Seniority level Mid-Senior level Employment type Contract Job function Information Technology Industries Information Technology & Services","['Databricks', 'BigQuery', 'Dataproc']",2024-04-21 15:53:59.643028
Amazon,"Data Engineer, Amazon",https://www.linkedin.com/jobs/view/data-engineer-amazon-at-amazon-3880458352?position=3&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=Ub7P69Cjjij6NorZIugoPQ%3D%3D&trk=public_jobs_jserp-result_search-card,"DescriptionWant to be part of an exciting journey of building the Next Generation Financial Systems Architecture to enable Amazon businesses across Retail, Digital, AWS, Physical stores, Corporate businesses, Amazon subsidiaries and many more?Want to shift your experiences from working at the peripheries of the systems to actually start owning and designing their cores?The Finance Automation team is looking for a Data Engineer to build our next generation of data engineering services in the Amazon finance space. The ideal candidate will be passionate about building large, scalable, and fast distributed systems on the AWS tech stack. The candidate will be part of a team with a goal to innovate and impact the entire finance business space within Amazon. We are one of the fastest growing Data Engineering teams across Amazon with technology orientation.We are looking for a candidate with background in the AWS technology stack (S3, Glue, IAM, Redshift) and data warehousing experience with interest in data mining and ability to identify emerging patterns and trends from large amounts of data. The candidate is expected to have strong experience with all standard data warehousing technical components (e.g. ETL, Reporting, and Data Modelling), infrastructure (hardware and software) and its integration. The candidate has experience in dimensional modelling, excellent problem-solving ability, capability in dealing with huge volumes of data, and a short learning curve. Written and verbal communication skills are required as the candidate will work very closely with different teams and senior leadership across Amazon.Along with complex problems to solve, we provide a world class work environment side by side with talented team members in the data engineering space and the opportunity to contribute and create history while having fun.Key job responsibilitiesDesign, build and own components of a high volume data warehouse.Build efficient data models using industry best practices and metadata for ad hoc and pre-built reporting.Interface with business customers, gathering requirements and delivering complete data and reporting solutions owning the design, development, and maintenance of ongoing metrics, reports, analyses, and dashboards to drive key business decisions.Continually improve ongoing reporting and analysis processes, automating and simplifying self-service support capabilities for our customers.Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources.Own the functional and non-functional scaling of software systems in your area.Provide input and recommendations on technical issues to other engineers, business stake holders, and data analysts.Collaborate with data scientists to continue to build and enhance new or existing ML programs.We are open to hiring candidates to work out of one of the following locations:Seattle, WA, USABasic Qualifications Experience as a data engineer or related specialty (e.g., software engineer, business intelligence engineer, data scientist) with a track record of manipulating, processing, and extracting value from large datasetsPreferred Qualifications Knowledge of batch and streaming data architectures like Kafka, Kinesis, Flink, Storm, BeamAmazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.Company - Amazon.com Services LLCJob ID: A2601916 Show more Show less Seniority level Not Applicable Employment type Full-time Job function Strategy/Planning, Analyst, and Information Technology Industries Software Development","['Redshift', 'Kafka', 'Kinesis']",2024-04-21 15:53:59.643028
Apexon,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-apexon-3844868840?position=4&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=hAe4zeyP4Y3hrHQ2sysYxA%3D%3D&trk=public_jobs_jserp-result_search-card,"Direct message the job poster from Apexon Mischelle Sharon Martis Mischelle Sharon Martis Talent Acquisition Manager at Apexon Job Overview We are looking for a Data Engineer to join our fast growing team. This person will be responsible for expanding and optimizing our current cloud based data pipeline architecture for the various cross functional teams. The ideal candidate has experience building robust data pipelines and reporting tools. However, someone with a strong application development background with keen interest in data engineering would be considered. The candidate will be collaborating extensively with Engineering, Analytics and Product teams to support functional use-cases and take data driven decisions. Who we are looking for Strong background in data processing & software engineering and can build high-quality, scalable data oriented products.Industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, EMR, etc..) for building efficient, large-scale data pipelines.Strong Software Engineering experience with in depth understanding of Python, Scala, Java or equivalentStrong understanding of data architecture, modeling and infrastructureExperience with building workflows (ETL pipelines)Experience with SQL and optimizing queriesProblem solver with attention to detail who can see complex problems in the data space through end to endWillingness to work in a fast paced environmentMS/BS in Computer Science or relevant industry experience.Strongly recommended (but optional) Experience building scalable applications on the Cloud (Amazon AWS, Google Cloud, etc..)Experience building stream-processing applications (Spark streaming, Apache-Flink, Kafka, etc..)Experience with Databricks and Snowflake Our Commitment to Diversity & Inclusion: Did you know that Apexon has been Certified™ by Great Place To Work®, the global authority on workplace culture, in each of the three regions in which it operates: USA (for the fourth time in 2023), India (seven consecutive certifications as of 2023), and the UK.Apexon is committed to being an equal opportunity employer and promoting diversity in the workplace. We take affirmative action to ensure equal employment opportunity for all qualified individuals. Apexon strictly prohibits discrimination and harassment of any kind and provides equal employment opportunities to employees and applicants without regard to gender, race, color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. You can read about our Job Applicant Privacy policy here Job Applicant Privacy Policy (apexon.com) Our Perks and Benefits: Our benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexer, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance. We also offer: o Health Insurance with Dental & Vision o 401K Plan o Life Insurance, STD & LTD o Paid Vacations & Holidays o Paid Parental Leave o FSA Dependent & Limited Purpose care Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Banking","['Snowflake', 'Databricks', 'Kafka', 'EMR']",2024-04-21 15:53:59.643028
Team Remotely Inc,Junior Data Engineer,https://www.linkedin.com/jobs/view/junior-data-engineer-at-team-remotely-inc-3907038188?position=5&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=eg7li9rHONIseAvDTRcUCQ%3D%3D&trk=public_jobs_jserp-result_search-card,"This is a remote position. Junior Data Engineer (1 year experience, remote)Be part of our future! This job posting builds our talent pool for potential future openings. We'll compare your skills and experience against both current and future needs. If there's a match, we'll contact you directly. No guarantee of immediate placement, and we only consider applications from US/Canada residents during the application process. Hiring Type: Full-Time Base Salary: $59K-$69K Per Annum.Position SummaryJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance the efficiency, reliability, and performance of CVS Health’s IT operations.Key Responsibilities include:Data pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. Data modeling: Create and maintain data models ensuring data quality, scalability, and efficiency Develop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency Data Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data Utilize big data technologies such as Kafka to process and analyze large volumes of data efficiently Implement data security measures to protect sensitive information and ensure compliance with data and privacy regulation Create/maintain documentation for data processes, data flows, and system configurations Performance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness Characteristics of this role:Team Player: Willing to teach, share knowledge, and work with others to make the team successful. Communication: Exceptional verbal, written, organizational, presentation, and communication skills. Creativity: Ability to take written and verbal requirements and come up with other innovative ideas. Attention to detail: Systematically and accurately research future solutions and current problems. Strong work ethic: The innate drive to do work extremely well. Passion: A drive to deliver better products and services than expected to customers. Required Qualifications2+ years of programming experience in languages such as Python, Java, SQL 2+ years of experience with ETL tools and database management (relational, non-relational) 2+ years of experience in data modeling techniques and tools to design efficient scalable data structures Skills in data quality assessment, data cleansing, and data validation Preferred QualificationsKnowledge of big data technologies and cloud platforms Experience with technologies like PySpark, Databricks, and Azure Synapse. EducationBachelor’s degree in Computer Science, Information Technology, or related field, or equivalent working experience Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Software Development","['Databricks', 'Kafka', 'NiFi', 'Synapse']",2024-04-21 15:53:59.643028
M Science,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-m-science-3876446098?position=6&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=8nwOwi48YMGaJQFxamYsWQ%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Steneral Consulting,SQL Data Engineer,https://www.linkedin.com/jobs/view/sql-data-engineer-at-steneral-consulting-3848323021?position=7&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=Gzu23UgslqQCHJ2JcnN6pw%3D%3D&trk=public_jobs_jserp-result_search-card,"Must-have skills: 5+ y of exp with interpreting database tables and data to understand correct extraction and mapping; SQL and SQL queries; exp with PLSQL Developer, SQL Plus, SQL Navigator, and AWS Glue; Migration exp ; Salesforce, AWS Redshift, and Snowflake; columnar databases.RequirementsJob Description: Experience with interpreting database tables and data to understand correct extraction and mapping (5+ years); Experience as a Data Engineer, Database Developer, or similar role; Experience with Structured Query Language (SQL); Experience creating, modifying, and querying relational databases; Experience in the creation of SQL queries to retrieve specific data from Needles by specifying various search criteria; Experience formatting, running totals, summaries, custom formulas, and custom Functions (SQL Queries); Experience with PLSQL Developer, SQL Plus, SQL Navigator, and AWS Glue; Experience with database-backed web applications; Experience with SQL and database management systems (e.g., MySQL, PostgreSQL, MongoDB); Experience with data manipulation languages and the principles of database design; Experience in software development and user interface web applications; Bachelor’s or Master’s degree in Computer Science, Information Technology, or a related field.Preferred Experience with both structured and unstructured data; Experience with both SQL and NoSQL databases; Experience with columnar databases; Migration experience; Experience with Salesforce, AWS Redshift, and Snowflake.Responsibilities Include But Are Not Limited To The Following Develop, optimize, test, and create extracts, integrations, and migrations of data to new and existing databases to help users retrieve data effectively; Work with technologies in the data field to support advanced analytics and data processing capabilities. Show more Show less Seniority level Mid-Senior level Employment type Contract Job function Information Technology Industries Software Development","['PostgreSQL', 'Snowflake', 'Redshift', 'MongoDB', 'MySQL']",2024-04-21 15:53:59.643028
Porter,Data Engineer,https://www.linkedin.com/jobs/view/data-engineer-at-porter-3882488631?position=8&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=JEfFE%2FB6vAavXrsScASBUw%3D%3D&trk=public_jobs_jserp-result_search-card,"Who We ArePorter seamlessly merges analytics with compassionate care to provide unparalleled healthcare coordination and optimization. Our platform empowers members with understanding, compassion, and peace of mind while leveraging robust AI analytics. Through our Care Guide team, we navigate members through the healthcare system, ensuring tailored support, comprehensive in-home assessments, and expert clinical services. By addressing care complexities, Porter enhances quality measures, reduces costs, and improves member experiences.Who You AreIn this role, you'll be responsible for creating innovative web and mobile applications, ensuring our front-end is responsive, efficient, and deployable across various public cloud regions. You'll apply your skills in developing both static and dynamic pages, integrating APIs with internal and external data sources, and managing sessions effectively.What You'll DoDevelop, maintain, and deliver technical documentation for ETL development, scheduled processes, frameworks, and transformations.Implement data cleaning transformations using ETL tooling, scripting, and data science techniques.Translate user stories into actionable tasks and participate in code reviews and design sessions within Agile processes.Monitor and troubleshoot existing ETL processes, enhancing or creating new ones as needed.Ensure the availability of data for reporting by monitoring and troubleshooting BI applications, datasets, frameworks, cubes, and reports.Utilize Agile methodologies to deliver features, increase velocity, and facilitate team communication and cohesion.Define and apply coding standards and best practices, promoting scalability, availability, and performance in technical designs.What You NeedRequired:5+ years of advanced SQL knowledge and experience with relational databases, including query authoring and familiarity with various database technologies.Proficiency in database technology, including index strategies, performance tuning, optimization, and stored procedures.Ability to perform root cause analysis on internal and external data and processes to address business questions and identify improvement opportunities.Strong analytical skills for working with structured and unstructured datasets.Extensive Python proficiency for advanced data manipulation, analysis, and visualization.Hands-on experience implementing databases such as NoSQL (e.g., DynamoDB, MongoDB) and relational databases (e.g., PostgreSQL, MySQL, Microsoft SQL).Expertise in building data models and executing complex queries using SQL.Experience in agile software methodologies.Bachelor's degree or equivalent experience/certification.Preferred:Experience with development in public cloud platforms like AWS or Azure.Highly valuable experience with Databricks, Spark, and big data technologies.Preference for candidates with experience in machine learning, data science, and statistical modeling.Healthcare experience is highly regarded.What We OfferCompetitive Salary Generous Stock Options Medical, Dental, and Vision (benefits within 30 days of hire)Fully Remote Paid vacation and holidaysA fun team and special culture Show more Show less Seniority level Associate Employment type Full-time Job function Engineering Industries Medical Practices, Hospitals and Health Care, and Veterinary Services","['PostgreSQL', 'Databricks', 'MongoDB', 'MySQL']",2024-04-21 15:53:59.643028
IntePros,Data Engineer II,https://www.linkedin.com/jobs/view/data-engineer-ii-at-intepros-3905333100?position=9&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=OyVCCh8nIdXBYiAjU8hV6w%3D%3D&trk=public_jobs_jserp-result_search-card,Job description not found or couldn't be loaded,[],2024-04-21 15:53:59.643028
Onbe,Associate Data Engineer,https://www.linkedin.com/jobs/view/associate-data-engineer-at-onbe-3900207053?position=10&pageNum=2&refId=3ZbjteuoCI1sz9qV4ktjNA%3D%3D&trackingId=LYzTwQl29OR2txjsjYOCFA%3D%3D&trk=public_jobs_jserp-result_search-card,"Onbe, a fast-growing FinTech, bringing innovation to a rapidly growing global marketplace, stands for “on behalf.” Because that’s exactly how we work: on behalf of our clients, as their comprehensive payments partner. We transform the way payments are imagined — as an opportunity for innovation, a source of insight to customers, and a way to connect with partners around the globe!Summary: Associate Data Engineer will be part of the Data Engineering/Technology organization building ETL pipelines for reporting and other analytics needs. The position requires strong SQL skills, knowledge of data warehouse concepts, exposure to data engineering tools and technologies, willingness to learn and burning desire to grow.This role is a hybrid role that will work onsite 1-2 days per week at our Buffalo Grove, IL location. Occasional travel may be required as part of this position.Responsibilities:Collaborate with business stakeholders and Senior Data Engineers to understand data requirements and translate into technical solutions.Perform data studies and data discovery around new or existing data sources.Review existing stored procedures for troubleshooting or optimization.Develop and maintain data pipelines from SQL Server and other data sources, Transform and load to Snowflake DB.Implement ETL process using Prophecy DBT tool to create facts and dimension tables.Collaborate with cross functional teams including reporting for scalable and reliable solutions.Document source to target mappings, data pipelines, transformations as part of knowledge sharing.Participate in daily status calls, design sessions and code reviews.Qualifications:Bachelor’s degree in computer science, Information Systems, or a related field.1-3 years of experience building data transformation and processing solutions using SQL, Python or similar programming languages.1-3 years of experience with ETL tools such as Azure Data Factory, Notebook, SQL, Python, SSIS.Ability to troubleshoot and resolve complex data related issues.Excellent communication and interpersonal skills, with willingness to collaborate and the desire to learn.Knowledge of Snowflake, SQL, SSRS, SSISKnowledge of Azure data platforms and related servicesThe base salary range for this position is between $74,880.00 to $85,000.00, with eligibility for an annual bonus. The actual base salary offered depends on a variety of factors, including but not limited to the qualifications of the individual applicant for the position, years of relevant experience, specific and unique skills, level of education attained, certifications or other professional licenses held, business needs, and market demand. Our competitive benefits includes medical, dental, vision, wellness, 401(k) matching, open time off, generous parental leave, and more! Our job titles may span more than one career level. All candidates are encouraged to apply.At Onbe, a diverse group of people, ideas, and perspectives are key to achieving phenomenal things. For over 25 years, our focus has remained on building a culture of openness and ingenuity, where employees come together to innovate and build disbursement solutions that make the lives of our clients and their consumers and workforces easier and better. Our definition of success includes celebrating differences and affirming belonging. To that end, we ask employees to come to Onbe as they are and contribute their diverse perspectives, identities, and experiences.We believe that the recruiting phase is only the very beginning of diversity and inclusion. At Onbe, we’re constantly evolving the way we celebrate diversity every day and in everything we do. With several internal committees that are dedicated to mental and physical wellness, diversity, inclusion, and community outreach, we are committed to making a culture that is inclusive to all.Onbe is proud to be an equal opportunity employer. We seek out ways to create a mindful workforce that embraces diversity and celebrates a culture of inclusion. We do not discriminate against employees or job applicants on the basis of race, color, ancestry, national origin, sex (including pregnancy), gender identity, sexual orientation, marital or family status, religion, age, disability, genetic information or military service. Our equal opportunity policy applies to all decisions of employment including hiring, placement, promotion or advancement, termination, layoff, recall, transfer, compensation, training and leaves of absence Show more Show less Seniority level Mid-Senior level Employment type Full-time Job function Information Technology Industries Financial Services","['Snowflake', 'dbt']",2024-04-21 15:53:59.643028
